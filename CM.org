#+TITLE : Prise de notes CM 4I100 ARCHI1
#+PROPERTY: header-args :mkdirp yes
#+STARTUP: inlineimages

Pirouz Bazargan-Sabet (pirouz.bazargan-sabet@lip6.fr)
partagé avec :
Karine Heydemann (karine.heydemann@lip6.fr)

4I100

* Informations pratiques

1 cours par semaine

1 TD de 4h (pas de machine)

3 parties distinctes :
- Processeurs
- Optimisation de code (Karine)
- Mémoire (organisation matérielle)

Deux examens répartis :

Examen réparti 1 (40%) : Processeurs et début de optimisation de code
Examen réparti 2 (60%) : Tout

Droit à tous les documents sous forme imprimée.

Pas de questions de cours ni de TD. On demande d'aller au-delà.

Annales disponibles, sans les corrigés.

UE réputée difficile, réputation imméritée. Moyenne des examens est autour de 8,5. 30% seulement réussissent en validation simple.
SESI obligatoire, SAR, RES, SFPN sont présents.

Support de cours apparemment compilé par les étudiants de ALIAS.

Hennessey-Patterson, _Computer architecture_
Il faut faire les annales, la structure des examens est très classique.

Le partiel portera sur les cours jusqu'au 4 compris.


* Cours 0 : 18/09/2019

Architecture et réalisation sont indépendants.

#+BEGIN_DEFINITION
Architecture concerne tous les aspects visibles de l'objet pour l'utilisateur. Tout ce que l'utilisateur doit connaître pour pouvoir en user.
#+END_DEFINITION

Qui est l'utilisateur dans le cas d'un processeur ?
[les processus, pas vraiment l'utilisateur de la machine]


#+BEGIN_DEFINITION
Réalisation concerne tous les aspects qu'on doit traiter pour concevoir l'objet.
#+END_DEFINITION

On doit être capable de concevoir les processeurs à la fin de l'UE.

Dans le cas des processeurs RISC, on ne peut pas parler d'architecture sans parler de réalisation (enfreinte de la règle plus haut).


Architecture représente 4 points (ce qu'un programmeur doit savoir du processeur pour écrire un programme en assembleur) :
- Registres visibles du logiciel
- Jeu d'instruction du processeur
- Comment le processeur voit la mémoire (l'abstraction de la mémoire vue du processeur)
- Mécanismes d'interruptions et reset


On va étudier le processeur MIPS-32.

Rupture de l'année 1980 (MIPS-32 et RISC 1) : conçues à Stanford et Berkeley respectivement.

Avant 1980 : Complex Instruction Set Computer (CISC)
Après 1980 : *Reduced* Instruction Set Computer (RISC)

** Registres de MIPS-32

*** Registres visibles

#+BEGIN_DEFINITION
Registre visible du logiciel.
Registre manipulable (lire ou écrire) avec une instruction assembleur
#+END_DEFINITION

Il y a 32 registres entiers visibles dans le MIPS-32
R0 à R31.

Chaque registre peut contenir 32 bits.

Aucune différence de permission entre ces registres, sauf R0 et R31.

R0 est le Trash Register \to pas vraiment un registre, c'est la constante 0. Il n'est pas modifiable

R31 est le Link Register. Permet d'enregistrer une adresse, l'adresse de retour de la fonction. On peut écrire dessus, même si c'est imprudent.

Ces registres sont indexés

*** Registres spéciaux

Deux registres nommés LO et HI (32 bits chacun), utilisés seulement pour l'opération * et /.

Le résultat d'une multiplication de deux entiers écrits sur 32 bits doit pouvoir s'écrire sur 64 bits maximum (preuve à refaire). Les deux registres mis bout-à-bout (HI puis LO) donnent le résultat.

Le résultat d'une division de deux entiers écrits sur 32 bits doit pouvoir s'écrire sur 32 : HI le reste, LO le quotient.

#+BEGIN_DEFINITION
Coprocesseur
Matériel dédié à un type d'opération particulière.
#+END_DEFINITION

Coprocesseur 0 gère le système d'exploitation. A besoin pour cela d'un certain nombre de registres (tous en 32 bits) qui lui sont réservées :
STATUS : Quelle est le mode de fonctionnement du processeur (USER ou KERNEL)
CAUSE : enregistre la cause de l'interruption ou de l'exception
EPC (EXCEPTION PROGRAM COUNTER) : adresse de l'instruction fautive
EBASE : Adresse du système d'exploitation dans la mémoire
BADVADDR : L'adresse (virtuelle) à laquelle le processeur voulait accéder avant erreur. Ne correspond pas forcément à une adresse physique de la machine.

** Jeu d'instruction

#+BEGIN_DEFINITION
Le langage assembleur n'utilise que le jeu d'instruction du processeur. Manipulé et écrit des humains. 

Le langage machine est en binaire. Manipulé par le processeur.

On a une traduction exacte et bijective entre le langage d'assemblage et le langage machine.

Un outil très simple permet de passer de l'un à l'autre, dans les deux sens.
#+END_DEFINITION

#+BEGIN_SRC asm
  add R3, R4, R5
#+END_SRC

Ici, l'instruction donne : "écrit dans R3 (la cible) la somme de R4 et R5".

*** Le langage machine

Dans le cas de RISC :

Toutes les instructions font la même taille : de cette manière, je sais où elles commencent et où elles s'arrêtent.

#+BEGIN_DEFINITION
Format d'instruction

Où je regarde dans les 32 bits pour trouver quelle ou quelle partie de l'instruction (où est la commande, où est la source 1, la source 2, la destination, etc... ?)
#+END_DEFINITION

On a trois formats dans un processeur MIPS-32 :

**** Le format régulier (R)

Dans un format régulier (R), on a :
- Un opcode : Code de l'opération qu'on veut faire, codée sur 6 bits, donc 2^6 opérations différentes (= 64). Innovation de RISC : permettre moins d'opérations.
- Le numéro du registre source Rs
- Le numéro du registre source Rs
- Le numéro du registre source Rs
- Le décalage éventuel
- Func, un complément du opcode

| Opcode | R_s | R_t | R_d | Shift Amount | Func |
|      6 |   5 |   5 |   5 |            5 |    6 |

Shift amount n'est utilisé que pour les instructions de décalage.

**** Le format immédiat (I)

Autre format, le format I (immédiat), pour les opérations avec des constantes :

| Opcode | R_s | R_t ou R_d | Const |
|      6 |   5 |          5 |    16 |

La constante est donc au maximum 2^16. Pour manipuler des plus grosses constantes, il faudra plusieurs instructions.

**** Le format jump (J)

Autre format, le format J (jump), pour les sauts :

| Opcode | Const |
|      6 |    26 |

La constante donne l'adresse vers laquelle on veut sauter. Ce processeur peut donc gérer 2^26 octets (64 Mo environ)

L'opcode est toujours au même endroit, parce que c'est ce qu'il faut pour déterminer quel est le format utilisé.

#+BEGIN_EXAMPLE
Le opcode 000000 (et 000001 apparemment) disent qu'on est sur un format R.
#+END_EXAMPLE

*** Le jeu d'instruction

Quatre catégories d'instruction :
- Instructions de calcul (arithmétiques et logiques)
- Instructions d'accès à la mémoire
- Instructions de contrôle (sauts ou branchements)
- Instructions dites système

**** Instructions calcul :

***** Addition (R) :

#+BEGIN_SRC asm
  Add Rd, Rs, Rt
#+END_SRC

Si le résultat de l'opération ne peut pas s'écrire sur 32 bits (33 maximum en cas d'addition de deux nombres sur 32 bits)
erreur d'overflow.

***** Addition U (R) :

#+BEGIN_SRC asm
  Addu Rd, Rs, Rt
#+END_SRC

Même chose sans erreur d'overflow

***** Sub (R)
Soustraction

***** Subu (R)
Même sans erreur d'overflow

***** addi (I)

#+BEGIN_SRC asm
  Addi Rd, Rd, I
#+END_SRC
 Addition du contenu d'un registre et d'une constante.

***** addiu (I)

La même sans erreur d'overflow.

Problème : On additione un entier sur 32 bits (le contenu d'un des 30 registres) et un entier sur 16 bits (les 16 derniers bits du mot).

Pour que cette opération soit valable, on doit convertir ce nombre écrit sur 16 bits en un nombre écrit en 32 (pas l'inverse, le registre qui doit accueillir le résultat étant grand de 32 bits)

*** Aparté : traduction d'un entier sur 16 bit vers 32 bits

Un certain nombre de choses sur lesquelles Pirouz "Ferrari" Bazargan est passé un peu vite.

#+BEGIN_THEOREM
Premier résultat :

$2^n = \sum_{i=O}^{n-1}(2^i) + 1$

Généralisable à :

$2^n = \sum_{i=q}^{n-1}(2^i) + 2^q$
#+END_THEOREM

#+BEGIN_PROOF
La démonstration est assez simple, elle se base sur les résultats des sommes de séries géométriques.

Soit la suite donnée par :

- $u_0 = 1$

- $u_{n+1} = 2 * u_n$

On dit que c'est une suite géométrique de raison 2, le terme général est donné par :

$u_n = u_0 * q^n$

À partir de là, on peut donner la somme de la série :

$S_n = \sum_{i=0}^{n} q^k = \frac{1-q^{n+1}}{1-q}$
Résultat supposé connu.

Il suffit juste de remplacer q par 2 dans la précédente équation et on a bien :

$2^n = \sum_{i=O}^{n-1}(2^i) + 1$

CQFD

Pour la généralisation, on a seulement besoin de casser la somme en deux :

$2^n = \sum_{i=O}^{n-1}(2^i) + 1 = \sum_{i=O}^{q-1}(2^i) + \sum_{i=q}^{n-1}(2^i) + 1$

Le premier et le dernier terme se somment en $2^q$

CQFD
#+END_PROOF

#+BEGIN_THEOREM
Deuxième résultat :

On peut écrire tous les entiers entre $0$ et $2^n - 1$ comme une combinaison binaire du vecteur $(2^{n-1}, 2^{n-2}, ...., 2^{0})$

ALITER :

$\forall i \in  [0 ; 2^n - 1], \exists \alpha$ un vecteur binaire (dont tous les éléments égalent 0 ou 1) tq :

$i = \sum_{k=0}^{n-1} (\alpha_{i} * 2^k)$
#+END_THEOREM

#+BEGIN_THEOREM
Corollaire :

On peut shift la range des nombres écrivables de l'intervalle $[0 ; 2^n - 1]$ à $[-2^{n-1} ; 2^{n-1} - 1]$ en changeant le vecteur à :

$(-2^{n-1}, 2^{n-2}, ...., 2^{0})$
#+END_THEOREM

Donc, si on veut écrire des nombres naturels, sans signe, on utilise le premier vecteur, si on veut écrire des nombres relatifs, on utilise le deuxième.

#+BEGIN_EXAMPLE
Donc, un même nombre en binaire : 1001, ne s'interprète pas de la même manière selon qu'on décide que c'est un entier naturel et un relatif :

Si c'est un naturel : 9
Si c'est un relatif : -7
#+END_EXAMPLE

#+BEGIN_THEOREM
Corollaire : Conversion

La conversion d'un nombre écrit sur n bits vers écrits sur n+k bits dépend de l'interprétation (naturel et relatif) :

Si c'est un naturel, il suffit de rajouter des 0 à gauche.
Si c'est un relatif, il faut rajouter le bit du poids fort à gauche.
#+END_THEOREM

#+BEGIN_PROOF
Soit un nombre naturel écrit sur n bits. On veut l'écrire sur n+k bits.

On a bien $\sum_{i=0}^{n-1}(\alpha_{i} * 2^i) = \sum_{i=0}^{n-1}(\alpha_{i} * 2^i) + \sum_{i=n}^{n+k-1}(0 * 2^i)$

Écrire des 0 à gauche fonctionne.

Soit un nombre relatif écrit sur n bits.

Vérifions que :

(1)$\sum_{i=0}^{n-2}(\alpha_{i} * 2^i) - \alpha_{n-1} * 2^{n-1} = 

\sum_{i=0}^{n-2}(\alpha_{i} * 2^i) +
\alpha_{n-1} * 2^{n-1} +
\sum_{i=n}^{n+k-2}(\alpha_{n-1} * 2^i) -
\alpha_{n-1} * 2^{n+k-1}
$

On rappelle que tous les $\alpha_i$ sont soit 0 soit 1. En particulier, on sait que $\alpha_{n-1}$ égale 0 ou 1.

Vérifions cette égalité pour $\alpha_{n-1} = 0$ :

Trivial.

Vérifions cette égalité pour $\alpha_{n-1} = 1$ :

On a :
(2) $2^{n+k-1} = \sum_{i=n}^{n+k-2} + 2^{n-1} + 2^{n-1}$
(Résultat plus haut) :

En injectant (2) dans (1), on a bien le premier terme qui s'annule, le deux derniers font changer le signe du $2^{n-1}$ de l'équation (1). L'égalité est vérifiée.

Donc, pour garder le même nombre relatif écrit sur n et sur n + k bits, il faut et il suffit de compléter à gauche du bit du poids fort la même valeur.

#+END_PROOF

On appelle les nombres dans Z les nombres arithmétiques, et les nombres de N de nombres logiques. (Jargon des architectes de processeur)

Puisque l'immédiat appartient à Z, on a pas besoin d'une instruction subi ou subiu (il suffit d'utiliser addi ou addiu avec un entier négatif).

*** Retour au jeu d'instructions

**** Suite des instructions calcul : les instuctions de décalage

***** SLL (Shift left logic) (R)

#+BEGIN_SRC asm
  sll Rd, Rt, Sham
#+END_SRC

Sham = Shift amount

Sham est codé sur 5 bits (on n'a que 32 registres). On peut donc se permettre de mettre cette instruction dans R.

Remarquez le Rt en lieu du Rs : on décale le deuxième registre source (pas de premier).

Cette opération met le contenu de Rt à gauche de Rd (les bits à gauche, autrement dit le poids fort). (Revient à multiplier par une puissance de 2 la partie de Rt qui n'est pas "écrasée", on décale les bits à gauche).

Dans le poids faible, on met des 0 : multiplication.

***** SRL (Shift Right Logic) (R)

#+BEGIN_SRC asm
  srl Rd, Rt, Sham
#+END_SRC

Sham = Shift amount

Sham est codé sur 5 bits. On peut donc se permettre de mettre cette instruction dans R.

Remarquez le Rt en lieu du Rs : on décale le deuxième registre source (pas de premier).

Cette opération met le contenu de Rt à droite de Rd (les bits à droite, autrement dit le poids faible).

Dans le poids fort, on complète avec des 0 : nombre logique.

***** SLA (Shift Right Arithmetic) (R)

#+BEGIN_SRC asm
  srl Rd, Rt, Sham
#+END_SRC

Pareil, avec des nombres arithmétiques (on étend le bit du poids fort si besoin est), et on complète avec des 0 à droite (multiplication par une puissance de 2).

***** SRA

#+BEGIN_SRC asm
  sra Rd, Rt, Sham
#+END_SRC

Pareil, avec nombres arithmétiques (on décale à droite de Sham octets), et on complète à gauche en étendant le bit du poids fort.

***** Or, And, Xor, Nor (R)
Prend trois registres Rd, Rs, Rt, et inscrit dans Rd le résultat de l'opération bit à bit OR, AND, XOR ou NOR (tous les 32 couples de bits sont interprétés et mis dans le bit correspondant du registre destination).

OR : On met 1 sssi au moins une des deux sources a 1
AND : On met 1 sssi les deux sources ont 1
XOR : On met 1 sssi exactement une source a 1
NOR : On met 1 sssi exactement zéro source a 1

***** Ori, Andi, Xori (I)
Même chose que la série précédente, avec un immédiat I

I est ici interprété comme un entier naturel (opération logique), il est donc étendu par zéro à 32 bits avant la comparaison.

#+BEGIN_QUOTE
The AND, OR, and XOR instructions can alternatively source one of the operands from a 16-bit immediate (which is zero-extended to 32 bits).

[[https://en.wikipedia.org/wiki/MIPS_architecture#ALU][Wikipedia MIPS]]
#+END_QUOTE

On a pas Nori :

La manière dont les architectes choisissent les opérations à inclure dans le jeu d'instruction dépendent du marché, des utilisateurs potentiels.
On fait des benchmark, on obtient une table des instructions du processeur utilisées, et leur poids.

Ici, Nori a dû être considéré pas assez important. De surcroît, c'est une opération de format I, et les places sont très chères (plus que dans R : Nor a été pris).

Si on part du principe qu'on peut réinterpréter une opération inexistante en N opérations existantes, on peut sacrifier cette opération à condition qu'elle soit peu utilisée.

#+BEGIN_THEOREM
Loi d'Amdhal

En ajoutant une instruction dans une machine, on a un gain. Le gain réel est bien entendu obtenu en tenant compte de la fréquence d'utilisation.

Gain effectif = Gain théorique * Fréquence d'utilisation
#+END_THEOREM

***** lui (I)

#+BEGIN_SRC asm
  lui Rd, I
#+END_SRC

Load upper immediate

Prend les 16 bits de I et les enregistre à gauche (poids fort) et on complète à droite (poids faible) avec des 0.

***** slt (R)

#+BEGIN_SRC asm
  slt Rd, Rs, Rt
#+END_SRC

Set on less than

Met 1 dans Rd sssi Rs < Rt (strictement), 0 sinon.
Le contenu de Rs et Rt sont interprétés comme des entiers signés.

***** Sltu (R)

#+BEGIN_SRC asm
  sltu Rd, Rs, Rt
#+END_SRC

Set on less than unsigned

Met 1 dans Rd sssi Rs < Rt (strictement), 0 sinon.

Le contenu de Rs et Rt sont interprétés comme des entiers non signés.

***** Slti (I)

#+BEGIN_SRC asm
  stli Rd, Rs, I
#+END_SRC

Set on less than immediate

Met 1 dans Rd sssi Rs < I (strictement), 0 sinon.

Le contenu de Rs et I sont interprétés comme des entiers signés.

***** sltiu I

#+BEGIN_SRC asm
  stliu Rd, Rs, I
#+END_SRC

Set on less than immediate unsigned

Met 1 dans Rd sssi Rs < I (strictement), 0 sinon.

Le contenu de Rs et I sont interprétés comme des entiers non signés.

#+BEGIN_QUOTE
The variants of these instructions that are suffixed with "unsigned" interpret the operands as unsigned integers (even those that source an operand from the sign-extended 16-bit immediate). 

[[https://en.wikipedia.org/wiki/MIPS_architecture#ALU][Wikipedia MIPS]]
#+END_QUOTE

**** Les instructions d'accès mémoire

Processeur MIPS est 32 bits, donc les adresses mémoire sont sur 32 bits.

1 adresse représente 1 octet.

On peut donc avoir 2^32 octets de mémoire, soit à peu près 4 Go.

2G (de l'espace d'adressage) sont réservés au système d'exploitation. Grâce au registre STATUS, on sait si le truc qui essaie d'accéder à la zone noyau de l'espace d'adressage est le noyau ou un utilisateur.

Important :
Il est ici question d'*espace d'adressage* !!!! Pas de mémoire physique. À un espace d'adressage de 4Go peut ne pas correspondre la même mémoire physique.

On peut lire ou écrire :
- octet
- Demi-mot (2 octets)
- Mot entier (4 octets)

***** Convention de cadrage

Les données sont cadrées à droite (convention). On met un octet dans le poids faible du registre (l'octet à droite).

***** Convention de boutage (endianness)

Quand tu copies vers la mémoire depuis un registre, dans quel sens : poids faible en haut (adresse plus petite) ou en bas (adresse plus grande) ?

Deux conventions :
- Little-endian (petit-boutiste) : Adresse la plus petite reçoit le poids fiable, la fin du mot
- Big-endian (gros-boutiste) : Adresse la plus grande reçoit le poids faible, la fin du mot

***** Convention des alignements des adresses

On ne peut lire que des adresses qui sont des multiples de la taille de l'objet.

L'adresse d'un octet est multiple de 1
L'adresse d'un demi-mot est multiple de 2
L'adresse d'un mot est multiple de 4

***** Lw (I)

#+BEGIN_SRC asm
  Lw Rd, I(Rs)
#+END_SRC

Lit 4 octets (load word) de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.

***** Sw (I)

#+BEGIN_SRC asm
  Sw Rt, I(Rs)
#+END_SRC

Store Word

Stocke 4 octets du registre Rt à l'adresse mémoire Rs + I.

***** LH

#+BEGIN_SRC asm
  LH Rd, I(Rs)
#+END_SRC

Lit 2 octets (load half-word) de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.
Serré à droite dans ce registre donc (convention de cadrage à droite).

Cette opération considère des entiers relatifs : on étend donc à gauche avec le signe.

***** LHU

#+BEGIN_SRC asm
  LHU Rd, I(Rs)
#+END_SRC

Lit 2 octets (load half-word) de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.
Serré à droite dans ce registre donc (convention de cadrage à droite).

Cette opération considère des entiers naturels : on étend donc à gauche avec des zéros.

***** SH

#+BEGIN_SRC asm
  SH Rt, I(Rs)
#+END_SRC

Store Half Word

Stocke 2 octets du registre Rt (les deux octets de droite, on suppose : convention) à l'adresse mémoire Rs + I.

***** LB

#+BEGIN_SRC asm
  LB Rd, I(Rs)
#+END_SRC

Load Byte

Lit 1 octet de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.
Serré à droite dans ce registre donc (convention de cadrage à droite).

Cette opération considère des entiers relatifs : on étend donc à gauche avec le signe.

***** LBU

#+BEGIN_SRC asm
  LBU Rd, I(Rs)
#+END_SRC

Load Byte Unsigned

Lit 1 octet de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.
Serré à droite dans ce registre donc (convention de cadrage à droite).

Cette opération considère des entiers naturels : on étend donc à gauche avec des zéros.

***** SB

#+BEGIN_SRC asm
  SB Rt, I(Rs)
#+END_SRC

Store Byte

Stocke 1 octet du registre Rt (l'octet de droite, on suppose) à l'adresse mémoire Rs + I.

**** Instructions de contrôle

***** Beq (I)

Branch if equal : Saute vers l'adresse "Label" si Rt à Rs

C'est l'assembleur qui traduit Label vers une adresse.

#+BEGIN_SRC asm
  Beq Rs, Rt, Label
#+END_SRC

Label est remplacé par un immédiat.

Si Rs != Rt, on continue à l'addresse suivante (@cible = @seq)
Si Rs = Rt, on (@cible = @Bt + 4 + I*4)

(Pourquoi +4 : On pense que c'est pour éviter une boucle infinie si I est donné à 0. On pourrait toujours donner I = -1, mais il faudrait le vouloir)

***** Bne (I)

Branch if ne

#+BEGIN_SRC asm
  Bne Rs, Rt, Label
#+END_SRC

***** BlTZ (I)
Branch if less than 0 (strict)

Compare Rs à 0 (pas besoin de Rt)

#+BEGIN_SRC asm
  BlTZ Rs, Label
#+END_SRC

***** BleZ (I)
Branch if less than 0 (large)

#+BEGIN_SRC asm
  BleZ Rs, Label
#+END_SRC

***** BgTZ (I)
Branch if greater than 0 (strict)

#+BEGIN_SRC asm
  BgTZ Rs, Label
#+END_SRC

***** BgeZ (I)
Branch if greater than 0 (large)

#+BEGIN_SRC asm
  BgeZ Rs, Label
#+END_SRC

***** J (J)

#+BEGIN_SRC asm
  J label
#+END_SRC

Branchement inconditionnel, soit saut.

Problème : On a que 26 bits pour mettre l'adresse vers laquelle on doit sauter.

On met :

- Les 4 (premiers) bits de l'adresse actuelle
- Les 26 bits du label
- 2 bits 00 au poids faible (en effet, si on saute vers un mot, l'adresse doit être multiple de 4. Et on sait qu'on saute vers un mot, puisqu'on saute vers une instruction.)

(On se rappellera de l'aparté plus haut :
A partir de cet aparté, on peut déduire trivialement que si un nombre *non-nul* écrit en binaire a ses n derniers chiffres égaux à 0, alors il est divisible par 2^n)

La partie variable de l'adresse de destination est de l'ordre de 2^28, pas de 2^32 (les 4 premiers bits fixes égaux aux 4 premiers bits de l'adresse actuelle). On ne peut sauter que dans un bloc (256 Mo) au lieu de pouvoir sauter dans l'espace d'adressage complet de \approx 4 Go

***** Jr (R)

Saute à l'adresse contenue dans un registre Rs.

#+BEGIN_SRC asm
  Jr Rs
#+END_SRC

***** Jal (J)

Jump and link. On ne pert pas l'endroit d'où on a sauté.

L'adresse de retour (l'adresse d'où on est parti + 4) est stockée dans R31.

#+BEGIN_SRC asm
  Jal Label
#+END_SRC

***** Jalr (R)

Jump and link, mais avec un registre Rs

#+BEGIN_SRC asm
  Jalr Rs
#+END_SRC


* Cours 1 : 19/09/2019


* Cours 2 : 26/09/2019

CISC vs RISC

#+BEGIN_DEFINITION
La micro-électronique naît avec la capacité qu'on a d'intégrer des fonctions sur des semi-conducteurs. On date sa naissance aux années 50.
#+END_DEFINITION

#+BEGIN_DEFINITION
Loi de Moore

Le nombre de transistors sur les circuits intégrés est censé doubler tous les 18 mois.
#+END_DEFINITION

#+BEGIN_DEFINITION
Transistor :

| Drain | Grille | Source |

La largeur de la grille détermine la tension entre le drain et la source. Plus on arrive à réduire la largeur de la grille, on peut augmenter le nombre de transistors.

Aujourd'hui la largeur de la grille minimale est de 7nm (elle était de l'ordre de 1 µm il y a 40 ans).
#+END_DEFINITION

CISC pensait que ces capacités supplémentaires serviront à faire des instructions de plus en plus complexes. Le but était de faire tendre l'assembleur CISC vers la complexité des langages de haut niveau. Le but était de réduire le *gap sémantique* entre les langages de haut niveau et l'assembleur CISC.

L'idée était que plus le langage assembleur est fort, plus il est facile d'exprimer des algorithmes complexes en un nombre réduit d'instructions processeur.

#+BEGIN_EXAMPLE
Le processeur IBM 370, datant de 1978, est l'exemple canonique du processeur CISC. Incluait une instruction strcmp (comparaison de chaîne de caractères).

Le processeur VAX (Virtual architecture extension) Digital (les inventeurs de la mémoire virtuelle). Dans le processeur VAX, on pouvait faire des additions avec des opérandes en mémoire (pas forcément dans les registres), avec le supplément d'instructions que ça supposait.

Chaque processeur était conçu pour un type d'application particulier.
#+END_EXAMPLE


L'intuition de RISC, c'est exactement le contraire. Il faut réduire les instructions, rapprocher l'assembleur du matériel.

*** Comparaison

Soit l'instruction C suivante :

#+BEGIN_SRC c
  a = b + c;
#+END_SRC

Les traductions en :

| VAX            | Mips           |
| ADD @a, @b, @c | LW R4, @b      |
|                | LW R5, @c      |
|                | ADD R6, R4, R5 |
|                | SW R6, @a      |

Si on retient comme critère le nombre d'instructions, VAX est objectivement mieux.

Il y a d'autres critères :

- L'encombrement mémoire (quel espace occupe le programme en mémoire avant d'être exécuté) : VAX est meilleur uniquement si la mémoire est chère.
- Facilité d'écrire les programmes : VAX est meilleur uniquement si on doit écrire en assembleur à la main.
- Time-to-market : on veut un processeur facile à faire, pour réduire le TTM (TTM du RISC = moins d'un an alors que TTM du CISC = 4 ans).

On peut choisir de faciliter la vie des gens qui fabriquent le matériel ou ceux qui écrivent les programmes en assembleur.
Si les deuxièmes disparaissent, on a plus besoin de choisir.

Le vrai sens des processeurs RISC, c'est Reject Important Stuff into the Compiler. C'était pensé comme une insulte de la part de CISC (dans un long papier du début des années 80), mais c'est en fait exactement l'idée, assumée par RISC : la production de l'assembleur est trop compliqué pour être laissé à des humains, ce sont les compilateurs qui doivent s'en occuper.

La conséquence logique de ça est donnée par une citation bien plus tardive (années 2000-2010) de Linus Torvalds :

#+BEGIN_QUOTE
Une architecture n'existe pas s'il n'existe pas de compilateur *C* vers cette architecture.

Linus Torvalds
#+END_QUOTE

*** Performance

La performance dépend de deux facteurs.

- La fréquence du processeur (F : Fréquence)
- Le nombre de cycles de la totalité des instructions à exécuter (CPI : Cycle Par Instruction), cappé à 1 bien entendu.

La performance est donc donnée par $\frac{F}{CPI}$

Pour se donner un objectif maximal sur la deuxième composante, il faut et il suffit d'atteindre ou de tendre vers CPI = 1.
On prend chacune des instructions, et on regarde ce qu'on doit mettre dans le processeur pour qu'elle soit exécutable en un seul cycle (si possible).

*Voilà les contraintes sur la réalisation*

|   |   |                              | ADD Rd, Rs, Rt | LW Rd, I(Rs) | SW | JR |
|---+---+------------------------------+----------------+--------------+----+----|
|   | D | Lire instruction en mémoire  | V              | V            | V  | V  |
|   | D | Décoder opcode               | V              | V            | V  | V  |
| D | D | Lire les opérandes           | V              | V            | V  | V  |
|   | D | Operation                    | V              | V            | V  |    |
|   | D | Accès mémoire                |                | V            | V  |    |
|   | X | Sauvegarde du résultat       | V              | V            |    |    |
| X |   | Adresse instruction suivante | V              | V            | V  | V  |

Le matériel est défini par la colonne de gauche : Ce que le matériel doit posséder pour pouvoir exécuter toutes les instructions possibles dans le jeu.

On doit maintenant regarder quelle opération dépend de quelle opération : graphe de dépendance.

Sauvegarde du résultat dépend de l'accès mémoire, qui lui même dépend de l'opération (quelle opération), qui lui-même dépend de l'opérande, qui dépend de quelle instruction on est en train d'exécuter, donc du décodage, qui dépend du chargement de l'instruction en mémoire.

Adresse de l'instruction suivante dépend de la lecture des opérandes.

La réalisation est en fait très simple : il faut et il suffit de construire une réalisation qui respecte les dépendances :

[[./CM2/realisation.png][Schéma de réalisation]]

On parle bien d'une boucle de conception : on ne part pas des instructions pour faire la réalisation, mais pas complètement l'inverse non plus.

On a bien CPI = 1, par construction (le CPI est défini comme le temps qu'il faut pour traverser le matériel qui est les opérations)

Comment on fait pour augmenter la fréquence à CPI défini et fixe ?

*** Pipeline

Notion de pipeline : au fond, chacun des ports du matériel peut être occupé au même moment. Si on met des registres entre les opérations atomiques, on augmente la fréquence. Il faut foutre des registres partout. Plus on découpe, plus on augmente la fréquence.

Le découpage en étage de pipeline n'a rien à voir avec les opérations : on n'est pas limité aux opérations, on peut couper en plein de portes, ou d'étage de pipeline.

La période d'horloge est défini comme l'opération la plus grande. Il faut couper de manière équilibrée.

#+BEGIN_THEOREM
Loi de pipeline :

- Les étages doivent être équilibrés
- Les étages doivent être séparés par des registres
- Le processeur doit disposer de deux accès à la mémoire : une pour les instructions, une pour les données (car il doit faire par cycle soit un soit deux accès à la mémoire : de toute façon on doit lire l'instruction, et ensuite on peut avoir à enregistrer des données.)
- On ne peut se servir d'un matériel qu'une fois par cycle
#+END_THEOREM

Les architectes du MIPS ont défini le pipeline comme ceci :

|   |   |                              | ADD Rd, Rs, Rt | LW Rd, I(Rs) | SW | JR | Etage de pipeline       |
|---+---+------------------------------+----------------+--------------+----+----+-------------------------|
|   | D | Lire instruction en mémoire  | V              | V            | V  | V  | IFC : Instruction fetch |
|   | D | Décoder opcode               | V              | V            | V  | V  | DEC : Decode            |
| D | D | Lire les opérandes           | V              | V            | V  | V  | DEC                     |
|   | D | Operation                    | V              | V            | V  |    | EXE : Execute           |
|   | D | Accès mémoire                |                | V            | V  |    | MEM : Memory access     |
|   | X | Sauvegarde du résultat       | V              | V            |    |    | WBK : Writeback         |
| X |   | Adresse instruction suivante | V              | V            | V  | V  |                         |

Pipeline :

| I | D     | E     | M     | W     |       |       |       |       |       |
|   | I + 1 | D + 1 | E + 1 | M + 1 | W + 1 |       |       |       |       |
|   |       | I + 2 | D + 2 | E + 2 | M + 2 | W + 2 |       |       |       |
|   |       |       | I + 3 | D + 3 | E + 3 | M + 3 | W + 3 |       |       |
|   |       |       |       | I + 4 | D + 4 | E + 4 | M + 4 | W + 4 |       |
|   |       |       |       |       | I + 5 | D + 5 | E + 5 | M + 5 | W + 5 |

Temps en abscisses (chaque trait est un front d'horloge : montant plus descendant)

Si ça correspond à peu près aux opérations, c'est par hasard : il se trouve que les étages étaient équilibrés de cette manière.

Le cycle d'instruction, c'est le temps qu'il faut pour injecter une nouvelle instruction (elle a été multipliée par 5 par notre amélioration)
La latence, c'est le temps que l'instruction met à se terminer (elle n'a pas changé, au moins pour la première opération)

On connaît l'adresse de l'instruction qui suit après D, mais on en a besoin avant ! En fait, l'adresse de l'instruction qui suit est l'adresse de l'instruction i + 2. L'adresse de l'instruction i + 1 est connue dès la fin de l'étage D de l'instruction n - 1.


* Cours 3 : 03/10/2019

Toutes les instructions passent par le même schéma d'exécution, qu'on ne rappellera pas ici.

C'est à la condition d'existence d'un schéma unique qu'on peut définir l'optimisation pipeline.

Règles du pipeline (rappels) :
- Les étages doivent être équilibrés : temps de propagation dans chaque étage doit être à peu près le même.
- Les étages doivent être séparés par des registres : les étages doivent être compartimentés.
- Un matéériel quelconque doit appartenir à un étage unique (tous les étages sont en train de travailler à chaque instant : un matériel ne peut pas faire deux choses à la fois)


Prenons une instruction simple, et regardons comment elle se comporte dans le pipeline : schéma détaillé, qui montre exactement ce qui se passe dans chaque étage quand j'exécute une instruction.

|          | IFC      |                | DEC    |          | EXE    |        | MEM      |         | WBK |             |
|----------+----------+----------------+--------+----------+--------+--------+----------+---------+-----+-------------|
|          |          |                |        |          |        |        | ->       | I_RM    | >-  |             |
|          |          |                |        |          | ->     | I_RE   | >-       |         |     |             |
|          |          |                | ->     | I_RD     | >-     |        |          |         |     |             |
|          | ->       | I_RI           | >-     |          |        |        |          |         |     |             |
|          |          |                | ->     | R Soper  | >-1    |        |          |         |     |             |
|          |          |                |        |          | 1+2 -> | RES_RE | >-       |         |     |             |
|          |          |                | ->     | R Toper  | >-2    |        |          |         |     |             |
|          |          |                |        | R Ioper  |        |        |          |         |     |             |
|          |          | 32 R du CPU    | >-     |          |        |        |          |         | ->  | 32 R du CPU |
|          |          |                |        |          |        |        | ->       | DATA_RM |     |             |
| R @instr | >-       | R @instruction | >-+4-> | R @instr |        |        |          |         |     |             |
|          | >-IMEM-> |                |        |          |        |        |          |         |     |             |
|          |          |                |        |          |        |        | >-DMEM-> |         |     |             |

Chaque trait représente un front d'horloge (montant, le trait descendant est au milieu de deux traits).

Ce genre de schéma permet de s'assurer que deux registres ne soient pas utilisés à deux moments différents.

On a deux matériels combinatoires (le truc dans EXE, et le +4 dans DEC) seulement, mais plein de registres. 70 % de la surface d'un processeur pipeline typique est consacrée aux registres.

Tous les registres sont suffixés par l'étage auquel ils appartiennent (un registre appartient à l'étage qui écrit dedans).

Le principe de ce schéma, c'est de lister le matériel nécessaire à faire une opération (ici, on a seulement dessiné pour ADD et LW).
En calquant les contraintes pour toutes les instructions, on obtient le schéma complet du matériel : métier de galérien. (et on parle de RISC, pas de CISC)

** Dépendance de branchement

La même chose, pour BEQ :

|          | IFC      |                | DEC       |          | EXE |   | MEM |   | WBK |   |
|----------+----------+----------------+-----------+----------+-----+---+-----+---+-----+---|
|          | ->       | I_RI           | >-        |          |     |   |     |   |     |   |
|          |          |                | ->        | R Soper  |     |   |     |   |     |   |
|          |          |                | 1=2       |          |     |   |     |   |     |   |
|          |          |                | ->        | R Toper  |     |   |     |   |     |   |
|          |          |                |           |          |     |   |     |   |     |   |
|          |          | 32 R du CPU    | >-1,2     |          |     |   |     |   |     |   |
|          |          |                |           |          |     |   |     |   |     |   |
| R @instr | >-       | R @instruction | >-+4 ->   | R @instr |     |   |     |   |     |   |
|          |          |                | >-+I*4 -> |          |     |   |     |   |     |   |
|          | >-IMEM-> |                |           |          |     |   |     |   |     |   |
|          |          |                |           |          |     |   |     |   |     |   |


Pour multiplier par une puissance de 2 (a + 2 * b), on peut se contenter de décaler la nappe de l'opérande b vers le poids fort de log_2(facteur)

La technique de calculer +(immédiat * 4) et +4 ne marche que si on sait que l'instruction précédente a bien demandé +4 : on n'a aucun moyen de s'en assurer, ce sera au compilateur de le faire : REJECT IMPORTANT STUFF into COMPILER.

On a un autre problème : au moment ou on a décidé qu'on devait aller ailleurs, l'instruction séquentielle est déjà chargée en registre. Comment on fait :
- On implémente une solution kill, mais ça coûte du matériel.
- On ne s'en occupe pas

L'instruction de branchement est retardée (Delayed Slot) : l'instruction séquentielle sera exécutée quoiqu'il arrive, ce qui n'est pas ce qu'on veut.

Une solution, c'est de mettre une opération NOP : certains compilateurs font ça.
Ou alors on trouve une chose utile à faire : on réarrange l'ordre des opérations : réordonnancement.

Deux contraintes pèsent sur le compilateur :

- L'instruction avant un branchement doit être séquentielle
- Il faut, autant qu'il est possible, mettre une opération utile dans l'opération qui suit le branchement.

gcc peut se voir demander plusieurs effort d'optimisation (-O). Mais même dans le cas où on lui demande de chercher partout, il n'est pas toujours capable de mettre qqch (25% du temps, il ne peut rien mettre)

Ici, le delayed slot est de 1 : on n'imagine même pas si c'est plus de 1. L'effort demandé au compilateur est encore pire (et impossible à fournir) : pour cette raison, les concepteurs du RISC tenaient à ce que l'adresse de l'instruction à exécuter ensuite soit calculée le plus tôt possible.

** Dépendance de données

On a un autre problème : un moment où une instruction doit consommer une valeur, elle peut ne pas encore avoir été calculée.

On a un délai nécessaire de 3 : c'est au compilateur de s'assurer de ça, sur n'importe quelle fenêtre de 4 instructions : toutes les instructions dans n'importe laquelle de ces fenêtres de 4 instructions doivent être indépendantes deux à deux.

Et ça le compilateur ne peut pas bien le faire.

On va quand même modifier le matériel.

Une idée, c'est de pouvoir bloquer une instruction en cours : injecter des cycles de gel (stall cycle) : pas mieux que la technique des NOP.

#+BEGIN_SRC mips
	  ADD         $3,$4,$5
	  ADD         $6,$7,$3
#+END_SRC

Dans cet exemple-là, ce n'est pas exactement de $3 dont on a besoin, mais du résultat de la somme $4+$5, qui est connue 2 cycles avant $3 (à la fin de EXE).

De la même manière, on a vraiment besoin de cette valeur au début de EXE, pas au début de DEC : il suffit de récupérer le contenu de RES_RE et le passer en deuxième opérande de l'opération +.

Donc en fait, le truc dont on a besoin ($4+$5) est disponible pile au moment où on en a besoin (au début de mon EXE, soit à la fin du EXE de mon t-1).

Soit en fait : (même si cette instruction n'existe pas, RES_RE n'étant pas un registre visible du processeur)

#+BEGIN_SRC mips
	  ADD         $3,$4,$5
	  ADD         $6,$7,$RES_RE
#+END_SRC

Cette technique s'appelle bypass.

Entre quelle zones peut-on/doit-on mettre des bypass ?

- E@t -> E@t+1
- M@t -> E@t+2
- E@t -> D@t+2
- M@t -> D@t+3

(* 2, car on a deux opérandes)


* Cours 4 : 10/10/2019

** Pipeline

#+BEGIN_DEFINITION
Pipeline

- Le traitement nécessaire à l'exécution d'un programme est découpé en étapes
- Un étage de pipeline par étape. On définit le cycle comme le temps nécessaire à la traversée de l'étage le plus long. Un étage, quel qu'il soit, est supposé durer un cycle (donc avoir des cycles plus rapides que d'autre ne sert à rien)
- A chaque cycle, une instruction commence son exécution, pas besoin d'attendre la fin de l'exécution précédente, qui prend donc (au moins) 5 cycles (5 étages).
#+END_DEFINITION

#+BEGIN_THEOREM
- Les étages fonctionnent en parallèle : Si l'étage IFC de l'instruction i est exécuté à un temps précis, on a en même temps l'étage DEC de l'instruction i-1, l'étage EXE de l'instruction i-2, etc...
- Les instructions entrent une par une dans le pipeline, dans l'ordre qui est donné par le programme
- Les instructions sortent une par une du pipeline, dans l'ordre qui est donné par le programme (le même ordre que celui dans lequel elles sont rentrées)
#+END_THEOREM

Idéalement, on aurait une instruction terminée à chaque cycle, soit un CPI de 1.


** Dépendances

#+BEGIN_DEFINITION
Deux instructions sont dites dépendantes si l'une doit être exécutée avant l'autre pour que le programme fasse bien ce qu'on demande.

On distingue les dépendances en deux types :
- Dépendances de données : opérandes en commun entre les deux instructions considérées
- Dépendances de contrôle : une des deux instructions est à exécuter seulement suivant le résultat de l'autre
#+END_DEFINITION

*** Dépendances de contrôle

#+BEGIN_DEFINITION
L'exécution d'une instruction i_2 située après une instruction i_1 dépend du résultat de cette dernière.

Le résultat d'un branchement (conditionné comme inconditionné) est connu à la fin de l'étage decode (DEC), soit un cycle *après* l'entrée de l'instruction séquentielle dans le pipeline.

On a donc un *delayed slot* après chaque branchement (conditionné comme inconditionné). Jugé plus économe que de flush lors de la fin de l'étage decode l'instruction entrée dans le pipeline alors qu'elle ne le devait pas (ce qui est fait sur des processeurs plus récents d'après Karine).

La performance est limitée : une instruction qui ne sert à rien occupe un cycle du processeur. (moins grave cependant qu'un cycle de gel, qui bloque tout le pipeline)
#+END_DEFINITION

*** Dépendances de données

On distingue trois cas de dépendances de données :
- La dépendance RAW : Read after Write (on écrit ça i_{1} \to_{RAW} i_{2}). Signifie que i_{1} écrit dans un registre et que i_{2} lit dans ce registre. Utilisation d'un résultat précédent.
- La dépendance WAW : Write after Write (on écrit ça i_{1} \to_{WAW} i_{2}). Signifie que i_{1} écrit dans un registre et que i_{2} écrit dans ce même registre. Réutilisation d'un registre.
- La dépendance WAR : Write after Read (on écrit ça i_{1} \to_{WAR} i_{2}). Signifie que i_{1} lit dans un registre et que i_{2} écrit dans ce registre. Réutilisation d'un registre.

#+BEGIN_THEOREM
Dans les trois cas considérés (*LES TROIS !*), si on inverse l'ordre dans lequel i_1 et i_2 sont exécutés, on change le sens ("la sémantique") du programme, on change en général ce que le programme fait (sauf si on a vraiment de la chance).
#+END_THEOREM

#+BEGIN_DEFINITION
On dit qu'on a un aléa dans le pipeline quand une donnée doit être *récupérée* (plutôt que consommée, attention) avant (au sens temporel du terme) sa production dans une des instructions précédente.

L'aléa ne peut être réglé que par l'introduction d'un cycle de gel à la place du cycle de pipeline de l'étage consommateur (répercuté bien entendu dans tous les étages de pipeline précédant celui-ci)
#+END_DEFINITION

#+BEGIN_THEOREM
Une dépendance de données n'introduit pas nécessairement d'aléa dans le pipeline. On peut parfaitement avoir une dépendance de données dans le code (ce qui signifie, on le rappelle, qu'intervertir les deux instructions changerait la sémantique du programme) sans que la production de la donnée de l'instruction avant arrive trop tard pour être récupérée à temps par l'instruction après.
#+END_THEOREM

Par exemple :

- WAW. L'écriture dans un registre est considérée finie à la fin de l'étage WBK. L'écriture suivante dans le même registre par n'importe quelle instruction future se fera forcément après (à la fin de l'étage WBK de l'instruction en question) :

| IFC | DEC | EXE | MEM | WBK (ici) |           |
|     | IFC | DEC | EXE | MEM       | WBK (ici) |

Pas de problème : et à plus forte raison si on considère une instruction située encore plus loin.

- WAR. La lecture des registres opérandes du banc de registre se fait dans l'étage DEC. L'écriture dans un de ces deux registres opérandes du banc de registre par n'importe quelle instruction future se fera forcément après (à la fin de l'étage WBK de l'instruction en question) :

| IFC | DEC (ici) | EXE | MEM | WBK |           |
|     | IFC       | DEC | EXE | MEM | WBK (ici) |

Pas de problème : et à plus forte raison si on considère une instruction située encore plus loin.

Par contre, les problèmes d'aléa dans le pipeline arrivent avec la dépendance de données RAW :

On a besoin de lire les registres opérandes du banc de registre au début de l'étage DEC. Or, si l'écriture dans ces registres se fait au WBK de l'instruction précédente, par exemple, ce ne sera pas prêt à temps :

| IFC | DEC | EXE       | MEM | WBK (ici) |     |
|     | IFC | (ici) DEC | EXE | MEM       | WBK |

Dans l'exemple donné, on a besoin de ce que WBK de l'instruction d'avant produit trois cycles avant qu'il soit effectivement produit.

Il faut trouver une manière d'accélérer la transmission des résultats aux opérandes !

Notion de *bypass*

*** Bypass

On a un nombre limité d'opérations dans le MIPS, regardons chaque opération distinctement :

ALU (Arithmetic and Logical Unit) :

#+BEGIN_SRC mips
	  ADD         $2, $4, $3
	  ADDI        $5, $2, 10
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage EXE, et on en a besoin au début de l'étage EXE de la seconde instruction (soit au même moment). Un bypass de EXE fin à EXE début règle le problème, et permet d'éviter le cycle de gel :

| IFC | DEC | EXE >-1 | MEM    | WBK |     |
|     | IFC | DEC     | -> EXE | MEM | WBK |

LOAD :

#+BEGIN_SRC mips
	  LW          $2, 0($4)
	  ADDI        $5, $2, 10
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage MEM, et on en a besoin au début de l'étage EXE de la seconde instruction (soit un cycle avant). Un bypass de MEM fin à EXE début diminue le problème, et permet de n'avoir à mettre qu'un cycle de gel :

| IFC | DEC | EXE | MEM >-1 | WBK    |     |     |
|     | IFC | DEC | O       | -> EXE | MEM | WBK |

BRANCH :

#+BEGIN_SRC mips
	  ADD         $2, $4, $3
	  BEQ         $5, $2, loop
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage EXE, et on en a besoin au début de l'étage DEC de la seconde instruction (soit un cycle avant). Un bypass de EXE fin à DEC début diminue le problème, et permet de n'avoir à mettre qu'un cycle de gel :

| IFC | DEC | EXE >-1 | MEM    | WBK |     |     |
|     | IFC | O       | -> DEC | EXE | MEM | WBK |

Deuxième exemple :

#+BEGIN_SRC mips
	  LW          $2, 0($3)          
	  BEQ         $5, $2, loop
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage MEM, et on en a besoin au début de l'étage DEC de la seconde instruction (soit deux cycles avant). Un bypass de MEM fin à DEC début diminue le problème, et permet de n'avoir à mettre que deux cycles de gel :

| IFC | DEC | EXE | MEM >-1 | WBK    |     |     |     |
|     | IFC | O   | O       | -> DEC | EXE | MEM | WBK |

STORE :

#+BEGIN_SRC mips
	  ADDI        $2, $3, 1
	  SW          $2, 0($4)
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage EXE, et on en a besoin au début de l'étage MEM de l'instruction suivante. On pourrait aussi envisager d'en avoir besoin au début de l'étage EXE de l'instruction suivante :

En effet, les opérations de type STORE consomment l'opérande RS (ici, $4) en EXE (pour calculer l'adresse à laquelle enregistrer), et l'opérande RT (ici $2) en MEM.

On a déjà un bypass de EXE fin à EXE début, donc le cas 2 est réglé.

Pour le cas 1, va-t-on mettre un bypass de EXE fin à MEM début ?
Non, pour deux raisons :
- C'est déjà le chemin naturel des données (une donnée passe de EXE fin à MEM début sans avoir besoin de quelque bypass) *dans notre cas*
- Raison plus générale : MEM est un étage critique (la criticité des étages, en décroissant : MEM, IF, DEC, EXE). On ne peut pas se permettre d'allonger encore la durée de l'étage MEM en y introduisant un bypass en entrée qui supposerait un multiplexeur et le matériel logique qui permettrait de le contrôler.

Pour cette raison, on ne met pas de bypass entrant en MEM : on préfèrera mettre un bypass entrant en EXE, même pour récupérer des données qui ne seront utilisées qu'en MEM.

Dans notre cas, ça n'induit pas de cycle de gel (on est dans le cas d'un bypass EXE fin vers EXE début, vu plus haut).

Mais, si on imagine un autre cas :

#+BEGIN_SRC mips
	  LW          $2, 0($3)
	  SW          $2, 0($4)
#+END_SRC

Dans ce cas, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage MEM, et on en a besoin au début de l'étage MEM de la seconde instruction. On pourrait imaginer un bypass qui irait de MEM fin à MEM début, qui permettrait de ne pas avoir à introduire un cycle de gel.

Mais, *PAS DE BYPASS ENTRANT EN MEM*, pour toutes les raisons données. On doit donc récupérer le résultat de l'opération LW (ici le contenu du registre $2) au début de l'étage EXE.

On se servira donc du bypass déjà vu : MEM fin -> EXE début, ce qui implique un cycle de gel.

Ce qui signifie qu'à un moment, les concepteurs du MIPS ont dû préférer introduire un cycle de gel plutôt que de payer un temps supplémentaire à l'étage MEM. En effet, MEM étant l'étage le plus gourmand en temps, ça aurait augmenté la durée du cycle pour toutes les instructions exécutées par ce processeur.


** Optimisation de code

Prenons l'exemple d'un code C simple :

#+BEGIN_SRC c
  int a[size];

  for (i = 0; i != size; ++i) a[i] = 2*a[i];
#+END_SRC

En code ASM MIPS compilé, ça donne ça :

#+BEGIN_SRC mips
	  OR          $0, $0, $0
					  # i dans R8
					  # size dans R6, a dans R5
	  XOR         $8, $8, $8
	  BEQ         $6, $0, suite
	  SLL         $9, $6, 2           #multiplication par 4
	  ADD         $9, $9, $5

  loop:
	  LW          $4, 0($5)
	  SLL         $7, $4, 1           #multiplication par 2
	  SW          $7, 0($5)
	  ADDIU       $5, $5, 4
	  BNE         $9, $5, loop

  suite:
#+END_SRC

Ce code tel quel ne respecte pas notre sémantique dans le MIPS 32, à cause du pipeline.

On doit déjà introduire des delayed slot derrière les branchements :

#+BEGIN_SRC mips
	  OR          $0, $0, $0
					  # i dans R8
					  # size dans R6, a dans R5
	  XOR         $8, $8, $8
	  BEQ         $6, $0, suite        
	  OR          $0, $0, $0          
	  SLL         $9, $6, 2           #multiplication par 4
	  ADD         $9, $9, $5

  loop:
	  LW          $4, 0($5)
	  SLL         $7, $4, 1           #multiplication par 2
	  SW          $7, 0($5)
	  ADDIU       $5, $5, 4
	  BNE         $9, $5, loop 
	  OR          $0, $0, $0

  suite:
#+END_SRC

On doit maintenant faire l'analyse de la performance de ce code assembleur. On se propose de faire le schéma simplifié, de manière à repérer les cycles de gel.

|       |   1 | 2   | 3   | 4   | 5   | 6       | 7      | 8   | 9       | 10         | 11     | 12      | 13     | 14  | 15  | 16  | 17  |
|-------+-----+-----+-----+-----+-----+---------+--------+-----+---------+------------+--------+---------+--------+-----+-----+-----+-----|
| XOR   | IFC | DEC | EXE | MEM | WBK |         |        |     |         |            |        |         |        |     |     |     |     |
| BEQ   |     | IFC | DEC | EXE | MEM | WBK     |        |     |         |            |        |         |        |     |     |     |     |
| NOP   |     |     | IFC | DEC | EXE | MEM     | WBK    |     |         |            |        |         |        |     |     |     |     |
| SLL   |     |     |     | IFC | DEC | EXE >-1 | MEM    | WBK |         |            |        |         |        |     |     |     |     |
| ADD   |     |     |     |     | IFC | DEC     | -> EXE | MEM | WBK     |            |        |         |        |     |     |     |     |
| LW    |     |     |     |     |     | IFC     | DEC    | EXE | MEM >-1 | WBK        |        |         |        |     |     |     |     |
| SLL   |     |     |     |     |     |         | IFC    | DEC | GEL     | -> EXE >-1 | MEM    | WBK     |        |     |     |     |     |
| SW    |     |     |     |     |     |         |        | IFC | GEL     | DEC        | -> EXE | MEM     | WBK    |     |     |     |     |
| ADDIU |     |     |     |     |     |         |        |     | GEL     | IFC        | DEC    | EXE >-1 | MEM    | WBK |     |     |     |
| BNE   |     |     |     |     |     |         |        |     |         |            | IFC    | GEL     | -> DEC | EXE | MEM | WBK |     |
| NOP   |     |     |     |     |     |         |        |     |         |            |        |         | IFC    | DEC | EXE | MEM | WBK |

Le CPI de cette suite d'instructions :

#Cycles = 17 - 5 = 12
#Instructions = 11
#Instructions_utiles = 9

CPI = 12/11
CPIutile = 12/9

2 cycles de gel. On peut sûrement faire mieux en changeant l'ordre des instructions (en veillant toutefois à ne pas changer la sémantique du programme).

*** Le réordonnancement

#+BEGIN_DEFINITION
Le réordonnancement des instructions suit plusieurs objectifs :
- Se débarrasser des cycles de gel
- Se débarrasser des instructions NOP

Et il doit respecter un certain nombre de contraintes :
- Il doit respecter toutes les dépendances (*pas seulement celles qui introduisent des cycles de gel*)
#+END_DEFINITION

Dans notre exemple, à quoi cette optimisation pourrait-elle ressembler ?

On décide de ne considérer que la boucle :

#+BEGIN_SRC mips
    loop:
	    LW          $4, 0($5)
	    SLL         $7, $4, 1           #multiplication par 2
	    SW          $7, 0($5)
	    ADDIU       $5, $5, 4
	    BNE         $9, $5, loop 
	    OR          $0, $0, $0
#+END_SRC

On rappelle :
- Un cycle de gel entre le LW et le SLL
- Un cycle de gel entre le ADDIU et le BNE
- Un NOP après le BNE, inutile

Le ADDIU incrémente $5 d'un pas constant : on peut faire remonter ADDIU avant SW sssi on corrige la modification qu'on fait sur $5 par l'offset de SW (-4 au lieu de 0).

Le ADDIU peut donc être remonté à la deuxième position, ce qui fait d'une pierre deux coups :
- On supprime le cycle de gel de SLL.
- On supprime le cycle de gel de BNE.

#+BEGIN_SRC mips
  loop:
	  lw          $4, 0($5)
	  addiu       $5, $5, 4
	  sll         $7, $4, 1           #multiplication par 2
	  sw          $7, -4($5)
	  bne         $9, $5, loop
	  or          $0, $0, $0
#+END_SRC

On respecte bien toutes les dépendances.

On peut maintenant mettre SW à la place du NOP, ce qui supprime l'instruction NOP :

#+BEGIN_SRC mips
  loop:
	  lw          $4, 0($5)
	  addiu       $5, $5, 4
	  sll         $7, $4, 1           #multiplication par 2
	  bne         $9, $5, loop
	  sw          $7, -4($5)
#+END_SRC

On a bien 0 cycles de gel et 0 instructions NOP.

Dans ce cycle de boucle en assembleur MIPS, on a deux instructions qui gèrent la boucle, et 3 instructions pour le corps. On a potentiellement moyen d'améliorer tout ça.

*** Déroulage de boucle

Mécaniquement, dérouler une boucle permet de diluer le coût de la gestion de la boucle (le coût en instructions assembleur d'une gestion de la boucle est constante), mais a aussi un autre avantage : le corps de la boucle devenant plus gros, on a plus d'opportunités de réordonnancement.

**** Aparté : le déroulage, pourquoi ne pas en abuser ?

Les problèmes liés au déroulage de boucle sont les suivants :

- Le segment de texte des processus va exploser. Pas un problème en soi (la mémoire ne coûte plus rien), mais peut devenir un problème pour le cache d'instructions (le IFC ne coûteront plus 0, il faudra faire des accès mémoire plus souvent que jamais : pourvu qu'on ne soit pas un prolo en cache, dans les processeurs modernes pour les codes pas trop gros, on peut quand même partir du principe qu'une grosse partie voir tout le texte du programme tient dans le cache : les IFC coûtent donc 0. Si on déroule le code, c'est moins possible)
- Le renommage des registres peut poser problème : si on regarde notre boucle déroulée, on a eu besoin de plus de registres différents. On n'a qu'un certain nombre de registres disponible.

Mais dans les faits, les compilateurs abusent du déroulement de boucle, surtout quand la taille de la boucle est statique : ne permet pas d'économiser des accès mémoire en écriture, ni même de les bien grouper (on n'a qu'un nombre limité de registres), suppose une légère augmentation des défauts de cache instruction, mais permet de virtuellement supprimer le coût des gestions de boucle (qui est somme toute minime, rappelons-le)

**** Retour à notre exemple

Essayons de dérouler la boucle :

à haut niveau :

#+BEGIN_SRC c
  for (i = 0; i + 1 < N; i+=2) {
	  tab[i] = tab[i] * 2;
	  tab[i+1] = tab[i+1] * 2;
  }

  for (; i < N; i++) {
	  tab[i] = tab[i] * 2;
  }
#+END_SRC

à bas niveau (en repartant de la boucle non encore optimisée) :

#+BEGIN_SRC mips
  loop:
	  lw          $4, 0($5)
	  sll         $7, $4, 1           #multiplication par 2
	  sw          $7, 0($5)    
	  lw          $14, 4($5)
	  sll         $17, $14, 1
	  sw          $17, 4($5)    

	  addiu       $5, $5, 4
	  bne         $9, $5, loop 
	  or          $0, $0, $0
#+END_SRC

Dans notre cas, on a seulement 2 instructions de gestion de boucle (on ne compte pas le NOP qui sera supprimé dans la suite) pour 6 et non plus 3 instructions de corps.

Dans cette version non encore optimisée, on a en revanche 3 cycles de gel et 1 instruction NOP.

De la même manière qu'avant, on peut supprimer les cycles et le NOP en réordonnançant. Sauf que cette fois, on a plus d'ordonnancement possibles, puisque le corps de la boucle est plus grand.

On doit prendre bien garde à respecter les dépendances :

#+BEGIN_SRC mips
  loop:
	  lw          $4, 0($5)
	  lw          $14, 4($5)
	  sll         $7, $4, 1           #multiplication par 2
	  sll         $17, $14, 1
	  addiu       $5, $5, 8
	  sw          $7, -8($5)    
	  bne         $9, $5, loop 
	  sw          $17, -4($5)  
#+END_SRC

On a bien 0 cycles de gel, et 0 instructions NOP.

On a 6 instructions de corps et 2 instructions de gestion de boucle : meilleur rapport.

Toutes ces techniques font bien mal à la tête ! On doit se donner un certain nombre d'outils théoriques qui permettront de formaliser correctement les possibilités d'optimisation, de manière à en évaluer leurs bénéfices et leur possibilité. (si on fait ça correctement, on pourra implémenter ces formalismes dans le compilateur, pour qu'il fasse les optimisations à notre place)

*** Optimisation et flot de contrôle

Si on prend un exemple un peu plus complexe :

#+BEGIN_SRC mips
  loop:
	  lw          $8, 0($5)
	  bgez        $8, endif
	  nop
	  sub         $9, $0, $8
	  sw          $9, 0($5)
  endif:
	  addiu       $5, $5, 4
	  bne         $7, $5, loop
	  nop
#+END_SRC

Dans notre cas, on a un branchement dans la boucle à l'instruction bgez (cas fréquent, traduit un simple if).

Quels problèmes pour l'ordonnancement et pour le déroulage de boucle ?

Il faut un des outils théoriques qui permettent de systématiser les réponses à cette question.

#+BEGIN_DEFINITION
Un *bloc de base* est une séquence d'instruction comportant un seul point d'entrée (la première instruction) et un seul point de sortie (la dernière), et qui suit la règle suivante :
Si la première instruction est exécutée, toutes les instructions du bloc de base le seront.
#+END_DEFINITION

#+BEGIN_THEOREM
On détermine les blocs de base d'un code de la façon suivante :

On détermine des en-têtes :

- La première instruction d'une fonction ou d'un bout de code
- L'instruction qui suit le dernier delayed slot après un saut
- L'instruction cible d'un saut

Une fois les en-tête placés, on peut définir les blocs de base de la manière suivante :
Du début à la fin du code, un bloc de base court d'une en-tête à la suivante exclue.
#+END_THEOREM

#+BEGIN_DEFINITION
Un graphe de contrôle de flot (CFG ou Control Flow Graph, dans la langue de 2Pac) est un graphe qui dessine les liens entre les blocs de base d'un programme. L'ensemble des liens constituent le contrôle de flot.

On dit qu'il y a un *arc* de BB1 vers BB2 deux blocs de base sssi :
- Soit il y a un saut de BB1 vers BB2
- Soit BB2 suit BB1 dans l'ordre du programme sans que BB1 ne se termine par un saut inconditionnel
#+END_DEFINITION

Si on reprend notre exemple :

#+BEGIN_SRC mips
					  #BB1
  loop:   
	  lw          $8, 0($5)
	  bgez        $8, endif
	  nop
					  #BB2
	  sub         $9, $0, $8
	  sw          $9, 0($5)
					  #BB3
  endif:  
	  addiu       $5, $5, 4
	  bne         $7, $5, loop
	  nop
#+END_SRC

L'ordonnancement doit prendre bien garde au flot de contrôle : on ne peut pas forcément bouger une instruction d'un bloc de base à un autre.

En revanche :

#+BEGIN_THEOREM
Si on a un seul bloc de base dans un code considéré, l'ordonnancement peut librement bouger n'importe laquelle instruction dans le bloc, pourvu que :
- Le code résultant respecte bien les dépendances
- Le code résultant soit bien toujours constitué d'un seul bloc de base
#+END_THEOREM

Dans le cas où on a plusieurs blocs de base, on doit faire plus attention aux arcs.

| BB1 | >-1 |
|-----+-----|
| V   |     |
|-----+-----|
| BB2 |     |
|-----+-----|
| V   |     |
|-----+-----|
| BB3 | <-1 |

Dans notre exemple :

- BB1 et BB3 sont toujours exécutés : on peut bouger des instructions de BB1 vers BB3 et vice versa, pourvu qu'on respecte les dépendances.
- BB2 n'est pas forcément exécuté : on ne peut pas mettre une instruction de BB1 dans BB2, ni remonter une instruction de BB3 de BB2
- On peut bouger des instructions dans BB2 (suit de la définition du bloc de base), et on peut *éventuellement* descendre des instructions de BB2 vers BB3, si on ne change pas la sémantique d'une itération.

Et comment fait-on quand on veut dérouler la boucle ?

#+BEGIN_SRC mips
					  #BB1
  loop:   
	  lw          $8, 0($5)
	  bgez        $8, endif
	  nop
					  #BB2
	  sub         $9, $0, $8
	  sw          $9, 0($5)
					  #BB1'
	  lw          $18, 4($5)
	  bgez        $18, endif
	  nop
					  #BB2'
	  sub         $19, $0, $8
	  sw          $9, 4($5)
					  #BB3
  endif:  
	  addiu       $5, $5, 4
	  bne         $7, $5, loop
	  nop
#+END_SRC

On a bien réécrit les blocs de contrôle.

On redessine le graphe :

| BB1  | >-1,2    |
|------+----------|
| V    |          |
|------+----------|
| BB2  |          |
|------+----------|
| V    |          |
|------+----------|
| BB1' | <-2, >-3 |
|------+----------|
| V    |          |
|------+----------|
| BB2' |          |
|------+----------|
| V    |          |
|------+----------|
| BB3  | <-1,3    |

On ne sépare pas un branchement interne de ses blocs successeurs.

On doit aussi faire attention à une dernière chose :

- Les instructions du corps de la boucle doivent être indépendantes les unes des autres, maintenant que leur indépendance n'est plus garantie par la boucle elle-même.
- On doit faire particulièrement aux modifications en avance.

En fait, le déroulage de boucle généralise la notion de "pipeline" : on traite des instructions de corps boucle à la pipeline : le traitement de la prochaine instruction de corps de boucle n'attend pas la prochaine itération de la boucle pour s'exécuter.


* Cours 5 : 17/10/2019

*** Rappels des épisodes précédents

Quelques rappels avant de voir la prochaine grande innovation des processeurs.

#+BEGIN_DEFINITION
La performance en temps d'exécution est donnée par la fréquence divisée par le CPI.
#+END_DEFINITION

#+BEGIN_THEOREM
On peut caractériser la dépendance de données de la manière suivante :

Si le cycle DEC d'un pipeline est situé avant l'écriture dans le banc de registre des registres qu'il doit lire, alors on a potentiellement un problème de dépendances de données.

Au contraire, si le cycle DEC est après cette écriture, on sait qu'on n'aura pas de problème de dépendances.

Si on a un pipeline à N étages, alors on peut avoir des problèmes de dépendances de l'instruction i à l'instruction i + n - 2 (on part du principe que DEC est le deuxième étage du pipeline). En particulier, dans notre cas d'un pipeline à 5 étages, on pourra avoir des problèmes de dépendances de données de i à i+3 au maximum.
#+END_THEOREM

#+BEGIN_THEOREM
Un cycle de gel augmente le nombre de cycles nécessaires à l'exécution d'un ensemble d'instructions de 1.
#+END_THEOREM

On a fondamentalement deux manières d'augmenter la performance, c'est d'augmenter la fréquence ou de diminuer le CPI.

L'idée du pipeline suit de la volonté d'augmenter la fréquence des processeurs. Les adaptations qu'on a vu en terme de bypass sont là pour préserver un CPI proche de 1.

On peut augmenter le fréquence, donc la performance en multipliant les étages de pipeline.

C'est l'invention du superpipeline.

*** Superpipeline

Mais comme on l'a vu avec le résultat plus haut, plus le pipeline est profond, plus on augmente le problème de la dépendance des données (celui-ci augmente linéairement en N le nombre d'étages du pipeline).

Augmenter le nombre de bypass n'est pas une solution miracle, car les bypass supposent un multiplexeur pour l'étage qui a le bypass en entrée, ce qui va avoir tendance à augmenter la durée du cycle, donc baisser la fréquence.

Autre problème, on augmente la quantité de delayed slots (CPI/CPI utile augmente) : si on dit que le décodage de l'adresse suivante se fait à la fin de l'étage N, on a N-1 delayed slots. (Déjà moins grave : on peut quand même partir du principe que l'étage DEC restera quoi qu'il en soit très près du début).

Si on interdit à l'utilisateur, donc au compilateur, de produire des branchements (pas de if ni de while autorisé), alors on peut se permettre de créer des processeurs avec beaucoup beaucoup d'étages de pipeline, qui seront efficaces pour le traitement des données peu dépendantes les unes des autres (typiquement, le calcul matriciel).

Le supercalculateur CRAY-2, de la fin des années 80 (1985).

*** Les processeurs SuperScalaire

On voit que les manières d'augmenter la fréquence en augmentant le nombre d'étages de pipeline posent presque autant de problème qu'elles en résolvent, et que les problèmes posés par cette solution n'augmentent pas de manière linéaire : à titre d'exemple, si un compilateur est assez capable de trouver une instruction à caser dans le delayed slot unique (75% du temps), ça devient beaucoup plus difficile avec 2 delayed slots (5%) et encore davantage avec 3 (0%).

Comment régler le problème donc ? Si on veut pouvoir continuer à augmenter la performance, mais que l'augmentation artificielle de la fréquence par le pipeline n'est pas assez rentable, il faut diminuer le CPI en dessous de ce qui était jusque là sa limite théorique : 1.

Autrement dit, il faut exécuter plusieurs instructions par cycle : invention des processus superscalaires, qui ont commencé à dominer dans les années 90. Les processeurs utilisés dans les machines modernes sont des processus superscalaires un peu particuliers (on le verra plus tard).

**** Aparté : les pipeline parallèles

L'idée d'avoir plusieurs pipeline n'est pas spécifique aux processeurs superscalaires. On sait que certaines opérations sont plus longues que d'autres : certains opérateurs comme l'addition, le shift, la comparaison bit à bit ont un temps de calcul algorithmique en le nombre de bits sur lequel ils sont appliqués, et certains autres (comme la multiplication) ne le sont pas du tout.

La manière naïve de faire une multiplication de deux nombres écrits sur 32 bits chacun est la manière de multiplier qu'on a appris à l'école : on fait les multiplications chiffre par chiffre (ici, bit à bit), puis on additionne le résultat des multiplications. On a au maximum 31 additions.

Un certain nombre d'algorithmes plus intelligents arrivent à faire ces multiplications en 3 additions plutôt qu'en 31. La multiplication reste quand même trois fois plus longue qu'une opération faite par l'unité logique et arithmétique.

On peut donc caser la multiplication à la places des 3 derniers étages du pipeline, en parallèle de ceux-ci.

**** Retour aux processus superscalaires

De la même manière que pour traiter la multiplication, on pourrait parfaitement doubler le pipeline d'exécution (E, M et W) pour en faire deux pipelines indépendants ?

Le cycle D devra donc décoder deux instructions à la fois.

Le cycle I doit maintenant alimenter le cycle decode avec deux instructions

La mémoire doit donc être capable de donner non pas 1, mais 2 instructions à la fois.

Ce qui suppose plusieurs choses :
- La largeur du bus CPU-mémoire centrale doit être doublée.
- Si on ne tient pas à mettre deux accès mémoire indépendants (ce qui coûte très cher), alors on doit demander au processeur de fetch deux mots d'instruction contigus d'un coup (les deux instructions pourront passer par le bus, mais ce sera toujours un seul bus, avec un seul contrôleur mémoire). Or, les mots d'instruction doivent être alignés (Adresses en octets divisibles par la taille du mot mémoire). L'adresse des couples d'instruction qu'on va demander à chercher devra donc être divisible par le double de la taille du mot mémoire. Si on veut que cette condition soit toujours respectée, on doit forcément :
-- Chercher toujours 2 mots d'instruction d'un coup
-- Ne jamais aller rechercher un mot d'instruction qu'on a déjà cherché avant (parmi les N qu'on va chercher) (donc ne jamais discard un mot d'instruction, à moins d'être sûr de ne point avoir envie de l'exécuter)

#+BEGIN_THEOREM
Problème généralisable à N : si on met N pipelines, le N-uplet d'instructions qu'on va chercher doit être divisible par N fois la taille du mot mémoire.

Si on veut que cette condition soit toujours respectée, on doit :
- Chercher toujours N mots d'instructions d'un coup
- Ne jamais aller rechercher un mot d'instruction qu'on a déjà cherché avant (parmi les N qu'on va chercher) (donc ne jamais discard un mot d'instruction, à moins d'être dûr de n'avoir point envie de l'exécuter)
#+END_THEOREM

Or, si les deux instructions passées chacune dans un des deux pipeline parallèle, il est parfaitement possible de ne pas pouvoir en exécuter une des deux (problème de dépendance). Or, d'après ce qu'on vient d'établir, on ne peut pas juste discard une instruction simplement parce qu'on ne peut pas l'exécuter : il faut être sûr de ne pas avoir envie du tout de l'exécuter, en raison par exemple d'un branchement conditionné ou non. Si on ne peut pas discard, il faut garder cette instruction non exécutée qqpart.

Cet endroit, c'est le *tampon d'instructions* (instruction buffer).

En fait, quand le matériel de l'étage IFC cherche deux mots d'instruction dans la mémoire, il place des deux mots d'instruction dans le tampon d'instruction, tampon dans lequel le matériel de l'étage DEC vient puiser une ou deux instructions (selon s'il peut en exécuter une ou deux).

#+BEGIN_THEOREM
Si on admet que le tampon ne peut être rempli que deux instructions par deux, et qu'on ne peut prélever que une ou deux instruction à la fois, quelle taille minimale du tampon garantit que l'étage DEC a toujours au moins deux instructions à prendre ?
4.

Résultat généralisable à N. Si on admet que le tampon ne peut être rempli que N instructions par N, et qu'on ne peut prélever que N, N-1, ..., 1 instruction à la fois, quelle taille minimale du tampon garantit que l'étage DEC a au moins N instructions à prendre ?
2N.
#+END_THEOREM

C'est la raison pour laquelle le tampon d'instruction peut tenir 4 instructions dans un processeur superscalaire à deux pipelines.

16 bypass par opérande, donc 32 en tout.

Le nombre de bypass est donné par : (2 * 2) ^ nombre de pipeline , par opérande.

Opération nop* fait pendant à l'instruction nop.

[reprendre le schéma après avoir trouvé un formalisme]

L'exécution se fait ici dans l'ordre : on bloque les deux pipeline.

Problème de l'alignement des adresses [réexpliquer] : au lieu de rejeter l'instruction qu'on ne peut pas exécuter de suite, on le met dans un tampon d'instruction (un fifo, qui doit avoir deux fois plus de place que le nombre de pipelines).

On peut toujours alimenter le buffer N par N, et on peut toujours prendre N instructions (Toutes les instruction fetch sont alignés, et on fait moins d'accès mémoire)

Doit particulièrement bien marcher avec le déroulage de boucle.

Par contre, les conditions d'optimisation sont beaucoup plus strictes.

[reprendre le calcul des cycles]

Il faut N accès à la mémoire des données.

Quand on met un cycle de gel dans un des N pipeline séparés (en E, M ou W), on perd 1/N

[refaire le superscalaire avec la boucle déroulée]

On peut compter le nombre de cycles en comptant la période à laquelle le tampon se vide à nouveau.

Au mieux, on peut exécuter n instructions en (n/N)+1 cycles, avec N le nombre de pipelines.


* Annexes

Support de cours :

[[./CM4/cours4.pdf][Cours Karine 1]]


