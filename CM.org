#+TITLE : Prise de notes CM 4I100 ARCHI1
#+PROPERTY: header-args :mkdirp yes
#+STARTUP: inlineimages

Pirouz Bazargan-Sabet (pirouz.bazargan-sabet@lip6.fr)
partagé avec :
Karine Heydemann (karine.heydemann@lip6.fr)

4I100

* Informations pratiques

1 cours par semaine

1 TD de 4h (pas de machine)

3 parties distinctes :
- Processeurs
- Optimisation de code (Karine)
- Mémoire (organisation matérielle)

Deux examens répartis :

Examen réparti 1 (40%) : Processeurs et début de optimisation de code
Examen réparti 2 (60%) : Tout

Droit à tous les documents sous forme imprimée.

Pas de questions de cours ni de TD. On demande d'aller au-delà.

Annales disponibles, sans les corrigés.

UE réputée difficile, réputation imméritée. Moyenne des examens est autour de 8,5. 30% seulement réussissent en validation simple.
SESI obligatoire, SAR, RES, SFPN sont présents.

Support de cours apparemment compilé par les étudiants de ALIAS.

Hennessey-Patterson, _Computer architecture_
Il faut faire les annales, la structure des examens est très classique.

Le partiel portera sur les cours jusqu'au 4 compris.

https://www-soc.lip6.fr/trac/sesi-m1archi/wiki


* Cours 0 : 18/09/2019

Architecture et réalisation sont indépendants.

#+BEGIN_DEFINITION
Architecture concerne tous les aspects visibles de l'objet pour l'utilisateur. Tout ce que l'utilisateur doit connaître pour pouvoir en user.
#+END_DEFINITION

Qui est l'utilisateur dans le cas d'un processeur ?
[les processus, pas vraiment l'utilisateur de la machine]


#+BEGIN_DEFINITION
Réalisation concerne tous les aspects qu'on doit traiter pour concevoir l'objet.
#+END_DEFINITION

On doit être capable de concevoir les processeurs à la fin de l'UE.

Dans le cas des processeurs RISC, on ne peut pas parler d'architecture sans parler de réalisation (enfreinte de la règle plus haut).


Architecture représente 4 points (ce qu'un programmeur doit savoir du processeur pour écrire un programme en assembleur) :
- Registres visibles du logiciel
- Jeu d'instruction du processeur
- Comment le processeur voit la mémoire (l'abstraction de la mémoire vue du processeur)
- Mécanismes d'interruptions et reset


On va étudier le processeur MIPS-32.

Rupture de l'année 1980 (MIPS-32 et RISC 1) : conçues à Stanford et Berkeley respectivement.

Avant 1980 : Complex Instruction Set Computer (CISC)
Après 1980 : *Reduced* Instruction Set Computer (RISC)

** Registres de MIPS-32

*** Registres visibles

#+BEGIN_DEFINITION
Registre visible du logiciel.
Registre manipulable (lire ou écrire) avec une instruction assembleur
#+END_DEFINITION

Il y a 32 registres entiers visibles dans le MIPS-32
R0 à R31.

Chaque registre peut contenir 32 bits.

Aucune différence de permission entre ces registres, sauf R0 et R31.

R0 est le Trash Register \to pas vraiment un registre, c'est la constante 0. Il n'est pas modifiable

R31 est le Link Register. Permet d'enregistrer une adresse, l'adresse de retour de la fonction. On peut écrire dessus, même si c'est imprudent.

Ces registres sont indexés

*** Registres spéciaux

Deux registres nommés LO et HI (32 bits chacun), utilisés seulement pour l'opération * et /.

Le résultat d'une multiplication de deux entiers écrits sur 32 bits doit pouvoir s'écrire sur 64 bits maximum (preuve à refaire). Les deux registres mis bout-à-bout (HI puis LO) donnent le résultat.

Le résultat d'une division de deux entiers écrits sur 32 bits doit pouvoir s'écrire sur 32 : HI le reste, LO le quotient.

#+BEGIN_DEFINITION
Coprocesseur
Matériel dédié à un type d'opération particulière.
#+END_DEFINITION

Coprocesseur 0 gère le système d'exploitation. A besoin pour cela d'un certain nombre de registres (tous en 32 bits) qui lui sont réservées :
STATUS : Quelle est le mode de fonctionnement du processeur (USER ou KERNEL)
CAUSE : enregistre la cause de l'interruption ou de l'exception
EPC (EXCEPTION PROGRAM COUNTER) : adresse de l'instruction fautive
EBASE : Adresse du système d'exploitation dans la mémoire
BADVADDR : L'adresse (virtuelle) à laquelle le processeur voulait accéder avant erreur. Ne correspond pas forcément à une adresse physique de la machine.

** Jeu d'instruction

#+BEGIN_DEFINITION
Le langage assembleur n'utilise que le jeu d'instruction du processeur. Manipulé et écrit des humains. 

Le langage machine est en binaire. Manipulé par le processeur.

On a une traduction exacte et bijective entre le langage d'assemblage et le langage machine.

Un outil très simple permet de passer de l'un à l'autre, dans les deux sens.
#+END_DEFINITION

#+BEGIN_SRC asm
  add R3, R4, R5
#+END_SRC

Ici, l'instruction donne : "écrit dans R3 (la cible) la somme de R4 et R5".

*** Le langage machine

Dans le cas de RISC :

Toutes les instructions font la même taille : de cette manière, je sais où elles commencent et où elles s'arrêtent.

#+BEGIN_DEFINITION
Format d'instruction

Où je regarde dans les 32 bits pour trouver quelle ou quelle partie de l'instruction (où est la commande, où est la source 1, la source 2, la destination, etc... ?)
#+END_DEFINITION

On a trois formats dans un processeur MIPS-32 :

**** Le format régulier (R)

Dans un format régulier (R), on a :
- Un opcode : Code de l'opération qu'on veut faire, codée sur 6 bits, donc 2^6 opérations différentes (= 64). Innovation de RISC : permettre moins d'opérations.
- Le numéro du registre source Rs
- Le numéro du registre source Rs
- Le numéro du registre source Rs
- Le décalage éventuel
- Func, un complément du opcode

| Opcode | R_s | R_t | R_d | Shift Amount | Func |
|      6 |   5 |   5 |   5 |            5 |    6 |

Shift amount n'est utilisé que pour les instructions de décalage.

**** Le format immédiat (I)

Autre format, le format I (immédiat), pour les opérations avec des constantes :

| Opcode | R_s | R_t ou R_d | Const |
|      6 |   5 |          5 |    16 |

La constante est donc au maximum 2^16. Pour manipuler des plus grosses constantes, il faudra plusieurs instructions.

**** Le format jump (J)

Autre format, le format J (jump), pour les sauts :

| Opcode | Const |
|      6 |    26 |

La constante donne l'adresse vers laquelle on veut sauter. Ce processeur peut donc gérer 2^26 octets (64 Mo environ)

L'opcode est toujours au même endroit, parce que c'est ce qu'il faut pour déterminer quel est le format utilisé.

#+BEGIN_EXAMPLE
Le opcode 000000 (et 000001 apparemment) disent qu'on est sur un format R.
#+END_EXAMPLE

*** Le jeu d'instruction

Quatre catégories d'instruction :
- Instructions de calcul (arithmétiques et logiques)
- Instructions d'accès à la mémoire
- Instructions de contrôle (sauts ou branchements)
- Instructions dites système

**** Instructions calcul :

***** Addition (R) :

#+BEGIN_SRC asm
  Add Rd, Rs, Rt
#+END_SRC

Si le résultat de l'opération ne peut pas s'écrire sur 32 bits (33 maximum en cas d'addition de deux nombres sur 32 bits)
erreur d'overflow.

***** Addition U (R) :

#+BEGIN_SRC asm
  Addu Rd, Rs, Rt
#+END_SRC

Même chose sans erreur d'overflow

***** Sub (R)
Soustraction

***** Subu (R)
Même sans erreur d'overflow

***** addi (I)

#+BEGIN_SRC asm
  Addi Rd, Rd, I
#+END_SRC
 Addition du contenu d'un registre et d'une constante.

***** addiu (I)

La même sans erreur d'overflow.

Problème : On additione un entier sur 32 bits (le contenu d'un des 30 registres) et un entier sur 16 bits (les 16 derniers bits du mot).

Pour que cette opération soit valable, on doit convertir ce nombre écrit sur 16 bits en un nombre écrit en 32 (pas l'inverse, le registre qui doit accueillir le résultat étant grand de 32 bits)

*** Aparté : traduction d'un entier sur 16 bit vers 32 bits

Un certain nombre de choses sur lesquelles Pirouz "Ferrari" Bazargan est passé un peu vite.

#+BEGIN_THEOREM
Premier résultat :

$2^n = \sum_{i=O}^{n-1}(2^i) + 1$

Généralisable à :

$2^n = \sum_{i=q}^{n-1}(2^i) + 2^q$
#+END_THEOREM

#+BEGIN_PROOF
La démonstration est assez simple, elle se base sur les résultats des sommes de séries géométriques.

Soit la suite donnée par :

- $u_0 = 1$

- $u_{n+1} = 2 * u_n$

On dit que c'est une suite géométrique de raison 2, le terme général est donné par :

$u_n = u_0 * q^n$

À partir de là, on peut donner la somme de la série :

$S_n = \sum_{i=0}^{n} q^k = \frac{1-q^{n+1}}{1-q}$
Résultat supposé connu.

Il suffit juste de remplacer q par 2 dans la précédente équation et on a bien :

$2^n = \sum_{i=O}^{n-1}(2^i) + 1$

CQFD

Pour la généralisation, on a seulement besoin de casser la somme en deux :

$2^n = \sum_{i=O}^{n-1}(2^i) + 1 = \sum_{i=O}^{q-1}(2^i) + \sum_{i=q}^{n-1}(2^i) + 1$

Le premier et le dernier terme se somment en $2^q$

CQFD
#+END_PROOF

#+BEGIN_THEOREM
Deuxième résultat :

On peut écrire tous les entiers entre $0$ et $2^n - 1$ comme une combinaison binaire du vecteur $(2^{n-1}, 2^{n-2}, ...., 2^{0})$

ALITER :

$\forall i \in  [0 ; 2^n - 1], \exists \alpha$ un vecteur binaire (dont tous les éléments égalent 0 ou 1) tq :

$i = \sum_{k=0}^{n-1} (\alpha_{i} * 2^k)$
#+END_THEOREM

#+BEGIN_THEOREM
Corollaire :

On peut shift la range des nombres écrivables de l'intervalle $[0 ; 2^n - 1]$ à $[-2^{n-1} ; 2^{n-1} - 1]$ en changeant le vecteur à :

$(-2^{n-1}, 2^{n-2}, ...., 2^{0})$
#+END_THEOREM

Donc, si on veut écrire des nombres naturels, sans signe, on utilise le premier vecteur, si on veut écrire des nombres relatifs, on utilise le deuxième.

#+BEGIN_EXAMPLE
Donc, un même nombre en binaire : 1001, ne s'interprète pas de la même manière selon qu'on décide que c'est un entier naturel et un relatif :

Si c'est un naturel : 9
Si c'est un relatif : -7
#+END_EXAMPLE

#+BEGIN_THEOREM
Corollaire : Conversion

La conversion d'un nombre écrit sur n bits vers écrits sur n+k bits dépend de l'interprétation (naturel et relatif) :

Si c'est un naturel, il suffit de rajouter des 0 à gauche.
Si c'est un relatif, il faut rajouter le bit du poids fort à gauche.
#+END_THEOREM

#+BEGIN_PROOF
Soit un nombre naturel écrit sur n bits. On veut l'écrire sur n+k bits.

On a bien $\sum_{i=0}^{n-1}(\alpha_{i} * 2^i) = \sum_{i=0}^{n-1}(\alpha_{i} * 2^i) + \sum_{i=n}^{n+k-1}(0 * 2^i)$

Écrire des 0 à gauche fonctionne.

Soit un nombre relatif écrit sur n bits.

Vérifions que :

(1)$\sum_{i=0}^{n-2}(\alpha_{i} * 2^i) - \alpha_{n-1} * 2^{n-1} = 

\sum_{i=0}^{n-2}(\alpha_{i} * 2^i) +
\alpha_{n-1} * 2^{n-1} +
\sum_{i=n}^{n+k-2}(\alpha_{n-1} * 2^i) -
\alpha_{n-1} * 2^{n+k-1}
$

On rappelle que tous les $\alpha_i$ sont soit 0 soit 1. En particulier, on sait que $\alpha_{n-1}$ égale 0 ou 1.

Vérifions cette égalité pour $\alpha_{n-1} = 0$ :

Trivial.

Vérifions cette égalité pour $\alpha_{n-1} = 1$ :

On a :
(2) $2^{n+k-1} = \sum_{i=n}^{n+k-2} + 2^{n-1} + 2^{n-1}$
(Résultat plus haut) :

En injectant (2) dans (1), on a bien le premier terme qui s'annule, le deux derniers font changer le signe du $2^{n-1}$ de l'équation (1). L'égalité est vérifiée.

Donc, pour garder le même nombre relatif écrit sur n et sur n + k bits, il faut et il suffit de compléter à gauche du bit du poids fort la même valeur.

#+END_PROOF

On appelle les nombres dans Z les nombres arithmétiques, et les nombres de N de nombres logiques. (Jargon des architectes de processeur)

Puisque l'immédiat appartient à Z, on a pas besoin d'une instruction subi ou subiu (il suffit d'utiliser addi ou addiu avec un entier négatif).

*** Retour au jeu d'instructions

**** Suite des instructions calcul : les instuctions de décalage

***** SLL (Shift left logic) (R)

#+BEGIN_SRC asm
  sll Rd, Rt, Sham
#+END_SRC

Sham = Shift amount

Sham est codé sur 5 bits (on n'a que 32 registres). On peut donc se permettre de mettre cette instruction dans R.

Remarquez le Rt en lieu du Rs : on décale le deuxième registre source (pas de premier).

Cette opération met le contenu de Rt à gauche de Rd (les bits à gauche, autrement dit le poids fort). (Revient à multiplier par une puissance de 2 la partie de Rt qui n'est pas "écrasée", on décale les bits à gauche).

Dans le poids faible, on met des 0 : multiplication.

***** SRL (Shift Right Logic) (R)

#+BEGIN_SRC asm
  srl Rd, Rt, Sham
#+END_SRC

Sham = Shift amount

Sham est codé sur 5 bits. On peut donc se permettre de mettre cette instruction dans R.

Remarquez le Rt en lieu du Rs : on décale le deuxième registre source (pas de premier).

Cette opération met le contenu de Rt à droite de Rd (les bits à droite, autrement dit le poids faible).

Dans le poids fort, on complète avec des 0 : nombre logique.

***** SLA (Shift Right Arithmetic) (R)

#+BEGIN_SRC asm
  srl Rd, Rt, Sham
#+END_SRC

Pareil, avec des nombres arithmétiques (on étend le bit du poids fort si besoin est), et on complète avec des 0 à droite (multiplication par une puissance de 2).

***** SRA

#+BEGIN_SRC asm
  sra Rd, Rt, Sham
#+END_SRC

Pareil, avec nombres arithmétiques (on décale à droite de Sham octets), et on complète à gauche en étendant le bit du poids fort.

***** Or, And, Xor, Nor (R)
Prend trois registres Rd, Rs, Rt, et inscrit dans Rd le résultat de l'opération bit à bit OR, AND, XOR ou NOR (tous les 32 couples de bits sont interprétés et mis dans le bit correspondant du registre destination).

OR : On met 1 sssi au moins une des deux sources a 1
AND : On met 1 sssi les deux sources ont 1
XOR : On met 1 sssi exactement une source a 1
NOR : On met 1 sssi exactement zéro source a 1

***** Ori, Andi, Xori (I)
Même chose que la série précédente, avec un immédiat I

I est ici interprété comme un entier naturel (opération logique), il est donc étendu par zéro à 32 bits avant la comparaison.

#+BEGIN_QUOTE
The AND, OR, and XOR instructions can alternatively source one of the operands from a 16-bit immediate (which is zero-extended to 32 bits).

[[https://en.wikipedia.org/wiki/MIPS_architecture#ALU][Wikipedia MIPS]]
#+END_QUOTE

On a pas Nori :

La manière dont les architectes choisissent les opérations à inclure dans le jeu d'instruction dépendent du marché, des utilisateurs potentiels.
On fait des benchmark, on obtient une table des instructions du processeur utilisées, et leur poids.

Ici, Nori a dû être considéré pas assez important. De surcroît, c'est une opération de format I, et les places sont très chères (plus que dans R : Nor a été pris).

Si on part du principe qu'on peut réinterpréter une opération inexistante en N opérations existantes, on peut sacrifier cette opération à condition qu'elle soit peu utilisée.

#+BEGIN_THEOREM
Loi d'Amdhal

En ajoutant une instruction dans une machine, on a un gain. Le gain réel est bien entendu obtenu en tenant compte de la fréquence d'utilisation.

Gain effectif = Gain théorique * Fréquence d'utilisation
#+END_THEOREM

***** lui (I)

#+BEGIN_SRC asm
  lui Rd, I
#+END_SRC

Load upper immediate

Prend les 16 bits de I et les enregistre à gauche (poids fort) et on complète à droite (poids faible) avec des 0.

***** slt (R)

#+BEGIN_SRC asm
  slt Rd, Rs, Rt
#+END_SRC

Set on less than

Met 1 dans Rd sssi Rs < Rt (strictement), 0 sinon.
Le contenu de Rs et Rt sont interprétés comme des entiers signés.

***** Sltu (R)

#+BEGIN_SRC asm
  sltu Rd, Rs, Rt
#+END_SRC

Set on less than unsigned

Met 1 dans Rd sssi Rs < Rt (strictement), 0 sinon.

Le contenu de Rs et Rt sont interprétés comme des entiers non signés.

***** Slti (I)

#+BEGIN_SRC asm
  stli Rd, Rs, I
#+END_SRC

Set on less than immediate

Met 1 dans Rd sssi Rs < I (strictement), 0 sinon.

Le contenu de Rs et I sont interprétés comme des entiers signés.

***** sltiu I

#+BEGIN_SRC asm
  stliu Rd, Rs, I
#+END_SRC

Set on less than immediate unsigned

Met 1 dans Rd sssi Rs < I (strictement), 0 sinon.

Le contenu de Rs et I sont interprétés comme des entiers non signés.

#+BEGIN_QUOTE
The variants of these instructions that are suffixed with "unsigned" interpret the operands as unsigned integers (even those that source an operand from the sign-extended 16-bit immediate). 

[[https://en.wikipedia.org/wiki/MIPS_architecture#ALU][Wikipedia MIPS]]
#+END_QUOTE

**** Les instructions d'accès mémoire

Processeur MIPS est 32 bits, donc les adresses mémoire sont sur 32 bits.

1 adresse représente 1 octet.

On peut donc avoir 2^32 octets de mémoire, soit à peu près 4 Go.

2G (de l'espace d'adressage) sont réservés au système d'exploitation. Grâce au registre STATUS, on sait si le truc qui essaie d'accéder à la zone noyau de l'espace d'adressage est le noyau ou un utilisateur.

Important :
Il est ici question d'*espace d'adressage* !!!! Pas de mémoire physique. À un espace d'adressage de 4Go peut ne pas correspondre la même mémoire physique.

On peut lire ou écrire :
- octet
- Demi-mot (2 octets)
- Mot entier (4 octets)

***** Convention de cadrage

Les données sont cadrées à droite (convention). On met un octet dans le poids faible du registre (l'octet à droite).

***** Convention de boutage (endianness)

Quand tu copies vers la mémoire depuis un registre, dans quel sens : poids faible en haut (adresse plus petite) ou en bas (adresse plus grande) ?

Deux conventions :
- Little-endian (petit-boutiste) : Adresse la plus petite reçoit le poids fiable, la fin du mot
- Big-endian (gros-boutiste) : Adresse la plus grande reçoit le poids faible, la fin du mot

***** Convention des alignements des adresses

On ne peut lire que des adresses qui sont des multiples de la taille de l'objet.

L'adresse d'un octet est multiple de 1
L'adresse d'un demi-mot est multiple de 2
L'adresse d'un mot est multiple de 4

***** Lw (I)

#+BEGIN_SRC asm
  Lw Rd, I(Rs)
#+END_SRC

Lit 4 octets (load word) de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.

***** Sw (I)

#+BEGIN_SRC asm
  Sw Rt, I(Rs)
#+END_SRC

Store Word

Stocke 4 octets du registre Rt à l'adresse mémoire Rs + I.

***** LH

#+BEGIN_SRC asm
  LH Rd, I(Rs)
#+END_SRC

Lit 2 octets (load half-word) de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.
Serré à droite dans ce registre donc (convention de cadrage à droite).

Cette opération considère des entiers relatifs : on étend donc à gauche avec le signe.

***** LHU

#+BEGIN_SRC asm
  LHU Rd, I(Rs)
#+END_SRC

Lit 2 octets (load half-word) de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.
Serré à droite dans ce registre donc (convention de cadrage à droite).

Cette opération considère des entiers naturels : on étend donc à gauche avec des zéros.

***** SH

#+BEGIN_SRC asm
  SH Rt, I(Rs)
#+END_SRC

Store Half Word

Stocke 2 octets du registre Rt (les deux octets de droite, on suppose : convention) à l'adresse mémoire Rs + I.

***** LB

#+BEGIN_SRC asm
  LB Rd, I(Rs)
#+END_SRC

Load Byte

Lit 1 octet de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.
Serré à droite dans ce registre donc (convention de cadrage à droite).

Cette opération considère des entiers relatifs : on étend donc à gauche avec le signe.

***** LBU

#+BEGIN_SRC asm
  LBU Rd, I(Rs)
#+END_SRC

Load Byte Unsigned

Lit 1 octet de mémoire à l'adresse Rs + I, enregistrés dans le registre Rd.
Serré à droite dans ce registre donc (convention de cadrage à droite).

Cette opération considère des entiers naturels : on étend donc à gauche avec des zéros.

***** SB

#+BEGIN_SRC asm
  SB Rt, I(Rs)
#+END_SRC

Store Byte

Stocke 1 octet du registre Rt (l'octet de droite, on suppose) à l'adresse mémoire Rs + I.

**** Instructions de contrôle

***** Beq (I)

Branch if equal : Saute vers l'adresse "Label" si Rt à Rs

C'est l'assembleur qui traduit Label vers une adresse.

#+BEGIN_SRC asm
  Beq Rs, Rt, Label
#+END_SRC

Label est remplacé par un immédiat.

Si Rs != Rt, on continue à l'addresse suivante (@cible = @seq)
Si Rs = Rt, on (@cible = @Bt + 4 + I*4)

(Pourquoi +4 : On pense que c'est pour éviter une boucle infinie si I est donné à 0. On pourrait toujours donner I = -1, mais il faudrait le vouloir)

***** Bne (I)

Branch if ne

#+BEGIN_SRC asm
  Bne Rs, Rt, Label
#+END_SRC

***** BlTZ (I)
Branch if less than 0 (strict)

Compare Rs à 0 (pas besoin de Rt)

#+BEGIN_SRC asm
  BlTZ Rs, Label
#+END_SRC

***** BleZ (I)
Branch if less than 0 (large)

#+BEGIN_SRC asm
  BleZ Rs, Label
#+END_SRC

***** BgTZ (I)
Branch if greater than 0 (strict)

#+BEGIN_SRC asm
  BgTZ Rs, Label
#+END_SRC

***** BgeZ (I)
Branch if greater than 0 (large)

#+BEGIN_SRC asm
  BgeZ Rs, Label
#+END_SRC

***** J (J)

#+BEGIN_SRC asm
  J label
#+END_SRC

Branchement inconditionnel, soit saut.

Problème : On a que 26 bits pour mettre l'adresse vers laquelle on doit sauter.

On met :

- Les 4 (premiers) bits de l'adresse actuelle
- Les 26 bits du label
- 2 bits 00 au poids faible (en effet, si on saute vers un mot, l'adresse doit être multiple de 4. Et on sait qu'on saute vers un mot, puisqu'on saute vers une instruction.)

(On se rappellera de l'aparté plus haut :
A partir de cet aparté, on peut déduire trivialement que si un nombre *non-nul* écrit en binaire a ses n derniers chiffres égaux à 0, alors il est divisible par 2^n)

La partie variable de l'adresse de destination est de l'ordre de 2^28, pas de 2^32 (les 4 premiers bits fixes égaux aux 4 premiers bits de l'adresse actuelle). On ne peut sauter que dans un bloc (256 Mo) au lieu de pouvoir sauter dans l'espace d'adressage complet de \approx 4 Go

***** Jr (R)

Saute à l'adresse contenue dans un registre Rs.

#+BEGIN_SRC asm
  Jr Rs
#+END_SRC

***** Jal (J)

Jump and link. On ne pert pas l'endroit d'où on a sauté.

L'adresse de retour (l'adresse d'où on est parti + 4) est stockée dans R31.

#+BEGIN_SRC asm
  Jal Label
#+END_SRC

***** Jalr (R)

Jump and link, mais avec un registre Rs

#+BEGIN_SRC asm
  Jalr Rs
#+END_SRC


* Cours 1 : 19/09/2019


* Cours 2 : 26/09/2019

CISC vs RISC

#+BEGIN_DEFINITION
La micro-électronique naît avec la capacité qu'on a d'intégrer des fonctions sur des semi-conducteurs. On date sa naissance aux années 50.
#+END_DEFINITION

#+BEGIN_DEFINITION
Loi de Moore

Le nombre de transistors sur les circuits intégrés est censé doubler tous les 18 mois.
#+END_DEFINITION

#+BEGIN_DEFINITION
Transistor :

| Drain | Grille | Source |

La largeur de la grille détermine la tension entre le drain et la source. Plus on arrive à réduire la largeur de la grille, on peut augmenter le nombre de transistors.

Aujourd'hui la largeur de la grille minimale est de 7nm (elle était de l'ordre de 1 µm il y a 40 ans).
#+END_DEFINITION

CISC pensait que ces capacités supplémentaires serviront à faire des instructions de plus en plus complexes. Le but était de faire tendre l'assembleur CISC vers la complexité des langages de haut niveau. Le but était de réduire le *gap sémantique* entre les langages de haut niveau et l'assembleur CISC.

L'idée était que plus le langage assembleur est fort, plus il est facile d'exprimer des algorithmes complexes en un nombre réduit d'instructions processeur.

#+BEGIN_EXAMPLE
Le processeur IBM 370, datant de 1978, est l'exemple canonique du processeur CISC. Incluait une instruction strcmp (comparaison de chaîne de caractères).

Le processeur VAX (Virtual architecture extension) Digital (les inventeurs de la mémoire virtuelle). Dans le processeur VAX, on pouvait faire des additions avec des opérandes en mémoire (pas forcément dans les registres), avec le supplément d'instructions que ça supposait.

Chaque processeur était conçu pour un type d'application particulier.
#+END_EXAMPLE


L'intuition de RISC, c'est exactement le contraire. Il faut réduire les instructions, rapprocher l'assembleur du matériel.

*** Comparaison

Soit l'instruction C suivante :

#+BEGIN_SRC c
  a = b + c;
#+END_SRC

Les traductions en :

| VAX            | Mips           |
| ADD @a, @b, @c | LW R4, @b      |
|                | LW R5, @c      |
|                | ADD R6, R4, R5 |
|                | SW R6, @a      |

Si on retient comme critère le nombre d'instructions, VAX est objectivement mieux.

Il y a d'autres critères :

- L'encombrement mémoire (quel espace occupe le programme en mémoire avant d'être exécuté) : VAX est meilleur uniquement si la mémoire est chère.
- Facilité d'écrire les programmes : VAX est meilleur uniquement si on doit écrire en assembleur à la main.
- Time-to-market : on veut un processeur facile à faire, pour réduire le TTM (TTM du RISC = moins d'un an alors que TTM du CISC = 4 ans).

On peut choisir de faciliter la vie des gens qui fabriquent le matériel ou ceux qui écrivent les programmes en assembleur.
Si les deuxièmes disparaissent, on a plus besoin de choisir.

Le vrai sens des processeurs RISC, c'est Reject Important Stuff into the Compiler. C'était pensé comme une insulte de la part de CISC (dans un long papier du début des années 80), mais c'est en fait exactement l'idée, assumée par RISC : la production de l'assembleur est trop compliqué pour être laissé à des humains, ce sont les compilateurs qui doivent s'en occuper.

La conséquence logique de ça est donnée par une citation bien plus tardive (années 2000-2010) de Linus Torvalds :

#+BEGIN_QUOTE
Une architecture n'existe pas s'il n'existe pas de compilateur *C* vers cette architecture.

Linus Torvalds
#+END_QUOTE

*** Performance

La performance dépend de deux facteurs.

- La fréquence du processeur (F : Fréquence)
- Le nombre de cycles de la totalité des instructions à exécuter (CPI : Cycle Par Instruction), cappé à 1 bien entendu.

La performance est donc donnée par $\frac{F}{CPI}$

Pour se donner un objectif maximal sur la deuxième composante, il faut et il suffit d'atteindre ou de tendre vers CPI = 1.
On prend chacune des instructions, et on regarde ce qu'on doit mettre dans le processeur pour qu'elle soit exécutable en un seul cycle (si possible).

*Voilà les contraintes sur la réalisation*

|   |   |                              | ADD Rd, Rs, Rt | LW Rd, I(Rs) | SW | JR |
|---+---+------------------------------+----------------+--------------+----+----|
|   | D | Lire instruction en mémoire  | V              | V            | V  | V  |
|   | D | Décoder opcode               | V              | V            | V  | V  |
| D | D | Lire les opérandes           | V              | V            | V  | V  |
|   | D | Operation                    | V              | V            | V  |    |
|   | D | Accès mémoire                |                | V            | V  |    |
|   | X | Sauvegarde du résultat       | V              | V            |    |    |
| X |   | Adresse instruction suivante | V              | V            | V  | V  |

Le matériel est défini par la colonne de gauche : Ce que le matériel doit posséder pour pouvoir exécuter toutes les instructions possibles dans le jeu.

On doit maintenant regarder quelle opération dépend de quelle opération : graphe de dépendance.

Sauvegarde du résultat dépend de l'accès mémoire, qui lui même dépend de l'opération (quelle opération), qui lui-même dépend de l'opérande, qui dépend de quelle instruction on est en train d'exécuter, donc du décodage, qui dépend du chargement de l'instruction en mémoire.

Adresse de l'instruction suivante dépend de la lecture des opérandes.

La réalisation est en fait très simple : il faut et il suffit de construire une réalisation qui respecte les dépendances :

[[./CM2/realisation.png][Schéma de réalisation]]

On parle bien d'une boucle de conception : on ne part pas des instructions pour faire la réalisation, mais pas complètement l'inverse non plus.

On a bien CPI = 1, par construction (le CPI est défini comme le temps qu'il faut pour traverser le matériel qui est les opérations)

Comment on fait pour augmenter la fréquence à CPI défini et fixe ?

*** Pipeline

Notion de pipeline : au fond, chacun des ports du matériel peut être occupé au même moment. Si on met des registres entre les opérations atomiques, on augmente la fréquence. Il faut foutre des registres partout. Plus on découpe, plus on augmente la fréquence.

Le découpage en étage de pipeline n'a rien à voir avec les opérations : on n'est pas limité aux opérations, on peut couper en plein de portes, ou d'étage de pipeline.

La période d'horloge est défini comme l'opération la plus grande. Il faut couper de manière équilibrée.

#+BEGIN_THEOREM
Loi de pipeline :

- Les étages doivent être équilibrés
- Les étages doivent être séparés par des registres
- Le processeur doit disposer de deux accès à la mémoire : une pour les instructions, une pour les données (car il doit faire par cycle soit un soit deux accès à la mémoire : de toute façon on doit lire l'instruction, et ensuite on peut avoir à enregistrer des données.)
- On ne peut se servir d'un matériel qu'une fois par cycle
#+END_THEOREM

Les architectes du MIPS ont défini le pipeline comme ceci :

|   |   |                              | ADD Rd, Rs, Rt | LW Rd, I(Rs) | SW | JR | Etage de pipeline       |
|---+---+------------------------------+----------------+--------------+----+----+-------------------------|
|   | D | Lire instruction en mémoire  | V              | V            | V  | V  | IFC : Instruction fetch |
|   | D | Décoder opcode               | V              | V            | V  | V  | DEC : Decode            |
| D | D | Lire les opérandes           | V              | V            | V  | V  | DEC                     |
|   | D | Operation                    | V              | V            | V  |    | EXE : Execute           |
|   | D | Accès mémoire                |                | V            | V  |    | MEM : Memory access     |
|   | X | Sauvegarde du résultat       | V              | V            |    |    | WBK : Writeback         |
| X |   | Adresse instruction suivante | V              | V            | V  | V  |                         |

Pipeline :

| I | D     | E     | M     | W     |       |       |       |       |       |
|   | I + 1 | D + 1 | E + 1 | M + 1 | W + 1 |       |       |       |       |
|   |       | I + 2 | D + 2 | E + 2 | M + 2 | W + 2 |       |       |       |
|   |       |       | I + 3 | D + 3 | E + 3 | M + 3 | W + 3 |       |       |
|   |       |       |       | I + 4 | D + 4 | E + 4 | M + 4 | W + 4 |       |
|   |       |       |       |       | I + 5 | D + 5 | E + 5 | M + 5 | W + 5 |

Temps en abscisses (chaque trait est un front d'horloge : montant plus descendant)

Si ça correspond à peu près aux opérations, c'est par hasard : il se trouve que les étages étaient équilibrés de cette manière.

Le cycle d'instruction, c'est le temps qu'il faut pour injecter une nouvelle instruction (elle a été multipliée par 5 par notre amélioration)
La latence, c'est le temps que l'instruction met à se terminer (elle n'a pas changé, au moins pour la première opération)

On connaît l'adresse de l'instruction qui suit après D, mais on en a besoin avant ! En fait, l'adresse de l'instruction qui suit est l'adresse de l'instruction i + 2. L'adresse de l'instruction i + 1 est connue dès la fin de l'étage D de l'instruction n - 1.


* Cours 3 : 03/10/2019

Toutes les instructions passent par le même schéma d'exécution, qu'on ne rappellera pas ici.

C'est à la condition d'existence d'un schéma unique qu'on peut définir l'optimisation pipeline.

Règles du pipeline (rappels) :
- Les étages doivent être équilibrés : temps de propagation dans chaque étage doit être à peu près le même.
- Les étages doivent être séparés par des registres : les étages doivent être compartimentés.
- Un matéériel quelconque doit appartenir à un étage unique (tous les étages sont en train de travailler à chaque instant : un matériel ne peut pas faire deux choses à la fois)


Prenons une instruction simple, et regardons comment elle se comporte dans le pipeline : schéma détaillé, qui montre exactement ce qui se passe dans chaque étage quand j'exécute une instruction.

|          | IFC      |                | DEC    |          | EXE    |        | MEM      |         | WBK |             |
|----------+----------+----------------+--------+----------+--------+--------+----------+---------+-----+-------------|
|          |          |                |        |          |        |        | ->       | I_RM    | >-  |             |
|          |          |                |        |          | ->     | I_RE   | >-       |         |     |             |
|          |          |                | ->     | I_RD     | >-     |        |          |         |     |             |
|          | ->       | I_RI           | >-     |          |        |        |          |         |     |             |
|          |          |                | ->     | R Soper  | >-1    |        |          |         |     |             |
|          |          |                |        |          | 1+2 -> | RES_RE | >-       |         |     |             |
|          |          |                | ->     | R Toper  | >-2    |        |          |         |     |             |
|          |          |                |        | R Ioper  |        |        |          |         |     |             |
|          |          | 32 R du CPU    | >-     |          |        |        |          |         | ->  | 32 R du CPU |
|          |          |                |        |          |        |        | ->       | DATA_RM |     |             |
| R @instr | >-       | R @instruction | >-+4-> | R @instr |        |        |          |         |     |             |
|          | >-IMEM-> |                |        |          |        |        |          |         |     |             |
|          |          |                |        |          |        |        | >-DMEM-> |         |     |             |

Chaque trait représente un front d'horloge (montant, le trait descendant est au milieu de deux traits).

Ce genre de schéma permet de s'assurer que deux registres ne soient pas utilisés à deux moments différents.

On a deux matériels combinatoires (le truc dans EXE, et le +4 dans DEC) seulement, mais plein de registres. 70 % de la surface d'un processeur pipeline typique est consacrée aux registres.

Tous les registres sont suffixés par l'étage auquel ils appartiennent (un registre appartient à l'étage qui écrit dedans).

Le principe de ce schéma, c'est de lister le matériel nécessaire à faire une opération (ici, on a seulement dessiné pour ADD et LW).
En calquant les contraintes pour toutes les instructions, on obtient le schéma complet du matériel : métier de galérien. (et on parle de RISC, pas de CISC)

** Dépendance de branchement

La même chose, pour BEQ :

|          | IFC      |                | DEC       |          | EXE |   | MEM |   | WBK |   |
|----------+----------+----------------+-----------+----------+-----+---+-----+---+-----+---|
|          | ->       | I_RI           | >-        |          |     |   |     |   |     |   |
|          |          |                | ->        | R Soper  |     |   |     |   |     |   |
|          |          |                | 1=2       |          |     |   |     |   |     |   |
|          |          |                | ->        | R Toper  |     |   |     |   |     |   |
|          |          |                |           |          |     |   |     |   |     |   |
|          |          | 32 R du CPU    | >-1,2     |          |     |   |     |   |     |   |
|          |          |                |           |          |     |   |     |   |     |   |
| R @instr | >-       | R @instruction | >-+4 ->   | R @instr |     |   |     |   |     |   |
|          |          |                | >-+I*4 -> |          |     |   |     |   |     |   |
|          | >-IMEM-> |                |           |          |     |   |     |   |     |   |
|          |          |                |           |          |     |   |     |   |     |   |


Pour multiplier par une puissance de 2 (a + 2 * b), on peut se contenter de décaler la nappe de l'opérande b vers le poids fort de log_2(facteur)

La technique de calculer +(immédiat * 4) et +4 ne marche que si on sait que l'instruction précédente a bien demandé +4 : on n'a aucun moyen de s'en assurer, ce sera au compilateur de le faire : REJECT IMPORTANT STUFF into COMPILER.

On a un autre problème : au moment ou on a décidé qu'on devait aller ailleurs, l'instruction séquentielle est déjà chargée en registre. Comment on fait :
- On implémente une solution kill, mais ça coûte du matériel.
- On ne s'en occupe pas

L'instruction de branchement est retardée (Delayed Slot) : l'instruction séquentielle sera exécutée quoiqu'il arrive, ce qui n'est pas ce qu'on veut.

Une solution, c'est de mettre une opération NOP : certains compilateurs font ça.
Ou alors on trouve une chose utile à faire : on réarrange l'ordre des opérations : réordonnancement.

Deux contraintes pèsent sur le compilateur :

- L'instruction avant un branchement doit être séquentielle
- Il faut, autant qu'il est possible, mettre une opération utile dans l'opération qui suit le branchement.

gcc peut se voir demander plusieurs effort d'optimisation (-O). Mais même dans le cas où on lui demande de chercher partout, il n'est pas toujours capable de mettre qqch (25% du temps, il ne peut rien mettre)

Ici, le delayed slot est de 1 : on n'imagine même pas si c'est plus de 1. L'effort demandé au compilateur est encore pire (et impossible à fournir) : pour cette raison, les concepteurs du RISC tenaient à ce que l'adresse de l'instruction à exécuter ensuite soit calculée le plus tôt possible.

** Dépendance de données

On a un autre problème : un moment où une instruction doit consommer une valeur, elle peut ne pas encore avoir été calculée.

On a un délai nécessaire de 3 : c'est au compilateur de s'assurer de ça, sur n'importe quelle fenêtre de 4 instructions : toutes les instructions dans n'importe laquelle de ces fenêtres de 4 instructions doivent être indépendantes deux à deux.

Et ça le compilateur ne peut pas bien le faire.

On va quand même modifier le matériel.

Une idée, c'est de pouvoir bloquer une instruction en cours : injecter des cycles de gel (stall cycle) : pas mieux que la technique des NOP.

#+BEGIN_SRC mips
	  ADD         $3,$4,$5
	  ADD         $6,$7,$3
#+END_SRC

Dans cet exemple-là, ce n'est pas exactement de $3 dont on a besoin, mais du résultat de la somme $4+$5, qui est connue 2 cycles avant $3 (à la fin de EXE).

De la même manière, on a vraiment besoin de cette valeur au début de EXE, pas au début de DEC : il suffit de récupérer le contenu de RES_RE et le passer en deuxième opérande de l'opération +.

Donc en fait, le truc dont on a besoin ($4+$5) est disponible pile au moment où on en a besoin (au début de mon EXE, soit à la fin du EXE de mon t-1).

Soit en fait : (même si cette instruction n'existe pas, RES_RE n'étant pas un registre visible du processeur)

#+BEGIN_SRC mips
	  ADD         $3,$4,$5
	  ADD         $6,$7,$RES_RE
#+END_SRC

Cette technique s'appelle bypass.

Entre quelle zones peut-on/doit-on mettre des bypass ?

- E@t -> E@t+1
- M@t -> E@t+2
- E@t -> D@t+2
- M@t -> D@t+3

(* 2, car on a deux opérandes)


* Cours 4 : 10/10/2019

** Pipeline

#+BEGIN_DEFINITION
Pipeline

- Le traitement nécessaire à l'exécution d'un programme est découpé en étapes
- Un étage de pipeline par étape. On définit le cycle comme le temps nécessaire à la traversée de l'étage le plus long. Un étage, quel qu'il soit, est supposé durer un cycle (donc avoir des cycles plus rapides que d'autre ne sert à rien)
- A chaque cycle, une instruction commence son exécution, pas besoin d'attendre la fin de l'exécution précédente, qui prend donc (au moins) 5 cycles (5 étages).
#+END_DEFINITION

#+BEGIN_THEOREM
- Les étages fonctionnent en parallèle : Si l'étage IFC de l'instruction i est exécuté à un temps précis, on a en même temps l'étage DEC de l'instruction i-1, l'étage EXE de l'instruction i-2, etc...
- Les instructions entrent une par une dans le pipeline, dans l'ordre qui est donné par le programme
- Les instructions sortent une par une du pipeline, dans l'ordre qui est donné par le programme (le même ordre que celui dans lequel elles sont rentrées)
#+END_THEOREM

Idéalement, on aurait une instruction terminée à chaque cycle, soit un CPI de 1.


** Dépendances

#+BEGIN_DEFINITION
Deux instructions sont dites dépendantes si l'une doit être exécutée avant l'autre pour que le programme fasse bien ce qu'on demande.

On distingue les dépendances en deux types :
- Dépendances de données : opérandes en commun entre les deux instructions considérées
- Dépendances de contrôle : une des deux instructions est à exécuter seulement suivant le résultat de l'autre
#+END_DEFINITION

*** Dépendances de contrôle

#+BEGIN_DEFINITION
L'exécution d'une instruction i_2 située après une instruction i_1 dépend du résultat de cette dernière.

Le résultat d'un branchement (conditionné comme inconditionné) est connu à la fin de l'étage decode (DEC), soit un cycle *après* l'entrée de l'instruction séquentielle dans le pipeline.

On a donc un *delayed slot* après chaque branchement (conditionné comme inconditionné). Jugé plus économe que de flush lors de la fin de l'étage decode l'instruction entrée dans le pipeline alors qu'elle ne le devait pas (ce qui est fait sur des processeurs plus récents d'après Karine).

La performance est limitée : une instruction qui ne sert à rien occupe un cycle du processeur. (moins grave cependant qu'un cycle de gel, qui bloque tout le pipeline)
#+END_DEFINITION

*** Dépendances de données

On distingue trois cas de dépendances de données :
- La dépendance RAW : Read after Write (on écrit ça i_{1} \to_{RAW} i_{2}). Signifie que i_{1} écrit dans un registre et que i_{2} lit dans ce registre. Utilisation d'un résultat précédent.
- La dépendance WAW : Write after Write (on écrit ça i_{1} \to_{WAW} i_{2}). Signifie que i_{1} écrit dans un registre et que i_{2} écrit dans ce même registre. Réutilisation d'un registre.
- La dépendance WAR : Write after Read (on écrit ça i_{1} \to_{WAR} i_{2}). Signifie que i_{1} lit dans un registre et que i_{2} écrit dans ce registre. Réutilisation d'un registre.

#+BEGIN_THEOREM
Dans les trois cas considérés (*LES TROIS !*), si on inverse l'ordre dans lequel i_1 et i_2 sont exécutés, on change le sens ("la sémantique") du programme, on change en général ce que le programme fait (sauf si on a vraiment de la chance).
#+END_THEOREM

#+BEGIN_DEFINITION
On dit qu'on a un aléa dans le pipeline quand une donnée doit être *récupérée* (plutôt que consommée, attention) avant (au sens temporel du terme) sa production dans une des instructions précédente.

L'aléa ne peut être réglé que par l'introduction d'un cycle de gel à la place du cycle de pipeline de l'étage consommateur (répercuté bien entendu dans tous les étages de pipeline précédant celui-ci)
#+END_DEFINITION

#+BEGIN_THEOREM
Une dépendance de données n'introduit pas nécessairement d'aléa dans le pipeline. On peut parfaitement avoir une dépendance de données dans le code (ce qui signifie, on le rappelle, qu'intervertir les deux instructions changerait la sémantique du programme) sans que la production de la donnée de l'instruction avant arrive trop tard pour être récupérée à temps par l'instruction après.
#+END_THEOREM

Par exemple :

- WAW. L'écriture dans un registre est considérée finie à la fin de l'étage WBK. L'écriture suivante dans le même registre par n'importe quelle instruction future se fera forcément après (à la fin de l'étage WBK de l'instruction en question) :

| IFC | DEC | EXE | MEM | WBK (ici) |           |
|     | IFC | DEC | EXE | MEM       | WBK (ici) |

Pas de problème : et à plus forte raison si on considère une instruction située encore plus loin.

- WAR. La lecture des registres opérandes du banc de registre se fait dans l'étage DEC. L'écriture dans un de ces deux registres opérandes du banc de registre par n'importe quelle instruction future se fera forcément après (à la fin de l'étage WBK de l'instruction en question) :

| IFC | DEC (ici) | EXE | MEM | WBK |           |
|     | IFC       | DEC | EXE | MEM | WBK (ici) |

Pas de problème : et à plus forte raison si on considère une instruction située encore plus loin.

Par contre, les problèmes d'aléa dans le pipeline arrivent avec la dépendance de données RAW :

On a besoin de lire les registres opérandes du banc de registre au début de l'étage DEC. Or, si l'écriture dans ces registres se fait au WBK de l'instruction précédente, par exemple, ce ne sera pas prêt à temps :

| IFC | DEC | EXE       | MEM | WBK (ici) |     |
|     | IFC | (ici) DEC | EXE | MEM       | WBK |

Dans l'exemple donné, on a besoin de ce que WBK de l'instruction d'avant produit trois cycles avant qu'il soit effectivement produit.

Il faut trouver une manière d'accélérer la transmission des résultats aux opérandes !

Notion de *bypass*

*** Bypass

On a un nombre limité d'opérations dans le MIPS, regardons chaque opération distinctement :

ALU (Arithmetic and Logical Unit) :

#+BEGIN_SRC mips
	  ADD         $2, $4, $3
	  ADDI        $5, $2, 10
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage EXE, et on en a besoin au début de l'étage EXE de la seconde instruction (soit au même moment). Un bypass de EXE fin à EXE début règle le problème, et permet d'éviter le cycle de gel :

| IFC | DEC | EXE >-1 | MEM    | WBK |     |
|     | IFC | DEC     | -> EXE | MEM | WBK |

LOAD :

#+BEGIN_SRC mips
	  LW          $2, 0($4)
	  ADDI        $5, $2, 10
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage MEM, et on en a besoin au début de l'étage EXE de la seconde instruction (soit un cycle avant). Un bypass de MEM fin à EXE début diminue le problème, et permet de n'avoir à mettre qu'un cycle de gel :

| IFC | DEC | EXE | MEM >-1 | WBK    |     |     |
|     | IFC | DEC | O       | -> EXE | MEM | WBK |

BRANCH :

#+BEGIN_SRC mips
	  ADD         $2, $4, $3
	  BEQ         $5, $2, loop
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage EXE, et on en a besoin au début de l'étage DEC de la seconde instruction (soit un cycle avant). Un bypass de EXE fin à DEC début diminue le problème, et permet de n'avoir à mettre qu'un cycle de gel :

| IFC | DEC | EXE >-1 | MEM    | WBK |     |     |
|     | IFC | O       | -> DEC | EXE | MEM | WBK |

Deuxième exemple :

#+BEGIN_SRC mips
	  LW          $2, 0($3)          
	  BEQ         $5, $2, loop
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage MEM, et on en a besoin au début de l'étage DEC de la seconde instruction (soit deux cycles avant). Un bypass de MEM fin à DEC début diminue le problème, et permet de n'avoir à mettre que deux cycles de gel :

| IFC | DEC | EXE | MEM >-1 | WBK    |     |     |     |
|     | IFC | O   | O       | -> DEC | EXE | MEM | WBK |

STORE :

#+BEGIN_SRC mips
	  ADDI        $2, $3, 1
	  SW          $2, 0($4)
#+END_SRC

Dans notre exemple, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage EXE, et on en a besoin au début de l'étage MEM de l'instruction suivante. On pourrait aussi envisager d'en avoir besoin au début de l'étage EXE de l'instruction suivante :

En effet, les opérations de type STORE consomment l'opérande RS (ici, $4) en EXE (pour calculer l'adresse à laquelle enregistrer), et l'opérande RT (ici $2) en MEM.

On a déjà un bypass de EXE fin à EXE début, donc le cas 2 est réglé.

Pour le cas 1, va-t-on mettre un bypass de EXE fin à MEM début ?
Non, pour deux raisons :
- C'est déjà le chemin naturel des données (une donnée passe de EXE fin à MEM début sans avoir besoin de quelque bypass) *dans notre cas*
- Raison plus générale : MEM est un étage critique (la criticité des étages, en décroissant : MEM, IF, DEC, EXE). On ne peut pas se permettre d'allonger encore la durée de l'étage MEM en y introduisant un bypass en entrée qui supposerait un multiplexeur et le matériel logique qui permettrait de le contrôler.

Pour cette raison, on ne met pas de bypass entrant en MEM : on préfèrera mettre un bypass entrant en EXE, même pour récupérer des données qui ne seront utilisées qu'en MEM.

Dans notre cas, ça n'induit pas de cycle de gel (on est dans le cas d'un bypass EXE fin vers EXE début, vu plus haut).

Mais, si on imagine un autre cas :

#+BEGIN_SRC mips
	  LW          $2, 0($3)
	  SW          $2, 0($4)
#+END_SRC

Dans ce cas, on a effectivement produit le résultat de l'opération de la première instruction à la fin de l'étage MEM, et on en a besoin au début de l'étage MEM de la seconde instruction. On pourrait imaginer un bypass qui irait de MEM fin à MEM début, qui permettrait de ne pas avoir à introduire un cycle de gel.

Mais, *PAS DE BYPASS ENTRANT EN MEM*, pour toutes les raisons données. On doit donc récupérer le résultat de l'opération LW (ici le contenu du registre $2) au début de l'étage EXE.

On se servira donc du bypass déjà vu : MEM fin -> EXE début, ce qui implique un cycle de gel.

Ce qui signifie qu'à un moment, les concepteurs du MIPS ont dû préférer introduire un cycle de gel plutôt que de payer un temps supplémentaire à l'étage MEM. En effet, MEM étant l'étage le plus gourmand en temps, ça aurait augmenté la durée du cycle pour toutes les instructions exécutées par ce processeur.


** Optimisation de code

Prenons l'exemple d'un code C simple :

#+BEGIN_SRC c
  int a[size];

  for (i = 0; i != size; ++i) a[i] = 2*a[i];
#+END_SRC

En code ASM MIPS compilé, ça donne ça :

#+BEGIN_SRC mips
	  OR          $0, $0, $0
					  # i dans R8
					  # size dans R6, a dans R5
	  XOR         $8, $8, $8
	  BEQ         $6, $0, suite
	  SLL         $9, $6, 2           #multiplication par 4
	  ADD         $9, $9, $5

  loop:
	  LW          $4, 0($5)
	  SLL         $7, $4, 1           #multiplication par 2
	  SW          $7, 0($5)
	  ADDIU       $5, $5, 4
	  BNE         $9, $5, loop

  suite:
#+END_SRC

Ce code tel quel ne respecte pas notre sémantique dans le MIPS 32, à cause du pipeline.

On doit déjà introduire des delayed slot derrière les branchements :

#+BEGIN_SRC mips
	  OR          $0, $0, $0
					  # i dans R8
					  # size dans R6, a dans R5
	  XOR         $8, $8, $8
	  BEQ         $6, $0, suite        
	  OR          $0, $0, $0          
	  SLL         $9, $6, 2           #multiplication par 4
	  ADD         $9, $9, $5

  loop:
	  LW          $4, 0($5)
	  SLL         $7, $4, 1           #multiplication par 2
	  SW          $7, 0($5)
	  ADDIU       $5, $5, 4
	  BNE         $9, $5, loop 
	  OR          $0, $0, $0

  suite:
#+END_SRC

On doit maintenant faire l'analyse de la performance de ce code assembleur. On se propose de faire le schéma simplifié, de manière à repérer les cycles de gel.

|       |   1 | 2   | 3   | 4   | 5   | 6       | 7      | 8   | 9       | 10         | 11     | 12      | 13     | 14  | 15  | 16  | 17  |
|-------+-----+-----+-----+-----+-----+---------+--------+-----+---------+------------+--------+---------+--------+-----+-----+-----+-----|
| XOR   | IFC | DEC | EXE | MEM | WBK |         |        |     |         |            |        |         |        |     |     |     |     |
| BEQ   |     | IFC | DEC | EXE | MEM | WBK     |        |     |         |            |        |         |        |     |     |     |     |
| NOP   |     |     | IFC | DEC | EXE | MEM     | WBK    |     |         |            |        |         |        |     |     |     |     |
| SLL   |     |     |     | IFC | DEC | EXE >-1 | MEM    | WBK |         |            |        |         |        |     |     |     |     |
| ADD   |     |     |     |     | IFC | DEC     | -> EXE | MEM | WBK     |            |        |         |        |     |     |     |     |
| LW    |     |     |     |     |     | IFC     | DEC    | EXE | MEM >-1 | WBK        |        |         |        |     |     |     |     |
| SLL   |     |     |     |     |     |         | IFC    | DEC | GEL     | -> EXE >-1 | MEM    | WBK     |        |     |     |     |     |
| SW    |     |     |     |     |     |         |        | IFC | GEL     | DEC        | -> EXE | MEM     | WBK    |     |     |     |     |
| ADDIU |     |     |     |     |     |         |        |     | GEL     | IFC        | DEC    | EXE >-1 | MEM    | WBK |     |     |     |
| BNE   |     |     |     |     |     |         |        |     |         |            | IFC    | GEL     | -> DEC | EXE | MEM | WBK |     |
| NOP   |     |     |     |     |     |         |        |     |         |            |        |         | IFC    | DEC | EXE | MEM | WBK |

Le CPI de cette suite d'instructions :

#Cycles = 17 - 5 = 12
#Instructions = 11
#Instructions_utiles = 9

CPI = 12/11
CPIutile = 12/9

2 cycles de gel. On peut sûrement faire mieux en changeant l'ordre des instructions (en veillant toutefois à ne pas changer la sémantique du programme).

*** Le réordonnancement

#+BEGIN_DEFINITION
Le réordonnancement des instructions suit plusieurs objectifs :
- Se débarrasser des cycles de gel
- Se débarrasser des instructions NOP

Et il doit respecter un certain nombre de contraintes :
- Il doit respecter toutes les dépendances (*pas seulement celles qui introduisent des cycles de gel*)
#+END_DEFINITION

Dans notre exemple, à quoi cette optimisation pourrait-elle ressembler ?

On décide de ne considérer que la boucle :

#+BEGIN_SRC mips
    loop:
	    LW          $4, 0($5)
	    SLL         $7, $4, 1           #multiplication par 2
	    SW          $7, 0($5)
	    ADDIU       $5, $5, 4
	    BNE         $9, $5, loop 
	    OR          $0, $0, $0
#+END_SRC

On rappelle :
- Un cycle de gel entre le LW et le SLL
- Un cycle de gel entre le ADDIU et le BNE
- Un NOP après le BNE, inutile

Le ADDIU incrémente $5 d'un pas constant : on peut faire remonter ADDIU avant SW sssi on corrige la modification qu'on fait sur $5 par l'offset de SW (-4 au lieu de 0).

Le ADDIU peut donc être remonté à la deuxième position, ce qui fait d'une pierre deux coups :
- On supprime le cycle de gel de SLL.
- On supprime le cycle de gel de BNE.

#+BEGIN_SRC mips
  loop:
	  lw          $4, 0($5)
	  addiu       $5, $5, 4
	  sll         $7, $4, 1           #multiplication par 2
	  sw          $7, -4($5)
	  bne         $9, $5, loop
	  or          $0, $0, $0
#+END_SRC

On respecte bien toutes les dépendances.

On peut maintenant mettre SW à la place du NOP, ce qui supprime l'instruction NOP :

#+BEGIN_SRC mips
  loop:
	  lw          $4, 0($5)
	  addiu       $5, $5, 4
	  sll         $7, $4, 1           #multiplication par 2
	  bne         $9, $5, loop
	  sw          $7, -4($5)
#+END_SRC

On a bien 0 cycles de gel et 0 instructions NOP.

Dans ce cycle de boucle en assembleur MIPS, on a deux instructions qui gèrent la boucle, et 3 instructions pour le corps. On a potentiellement moyen d'améliorer tout ça.

*** Déroulage de boucle

Mécaniquement, dérouler une boucle permet de diluer le coût de la gestion de la boucle (le coût en instructions assembleur d'une gestion de la boucle est constante), mais a aussi un autre avantage : le corps de la boucle devenant plus gros, on a plus d'opportunités de réordonnancement.

**** Aparté : le déroulage, pourquoi ne pas en abuser ?

Les problèmes liés au déroulage de boucle sont les suivants :

- Le segment de texte des processus va exploser. Pas un problème en soi (la mémoire ne coûte plus rien), mais peut devenir un problème pour le cache d'instructions (le IFC ne coûteront plus 0, il faudra faire des accès mémoire plus souvent que jamais : pourvu qu'on ne soit pas un prolo en cache, dans les processeurs modernes pour les codes pas trop gros, on peut quand même partir du principe qu'une grosse partie voir tout le texte du programme tient dans le cache : les IFC coûtent donc 0. Si on déroule le code, c'est moins possible)
- Le renommage des registres peut poser problème : si on regarde notre boucle déroulée, on a eu besoin de plus de registres différents. On n'a qu'un certain nombre de registres disponible.

Mais dans les faits, les compilateurs abusent du déroulement de boucle, surtout quand la taille de la boucle est statique : ne permet pas d'économiser des accès mémoire en écriture, ni même de les bien grouper (on n'a qu'un nombre limité de registres), suppose une légère augmentation des défauts de cache instruction, mais permet de virtuellement supprimer le coût des gestions de boucle (qui est somme toute minime, rappelons-le)

**** Retour à notre exemple

Essayons de dérouler la boucle :

à haut niveau :

#+BEGIN_SRC c
  for (i = 0; i + 1 < N; i+=2) {
	  tab[i] = tab[i] * 2;
	  tab[i+1] = tab[i+1] * 2;
  }

  for (; i < N; i++) {
	  tab[i] = tab[i] * 2;
  }
#+END_SRC

à bas niveau (en repartant de la boucle non encore optimisée) :

#+BEGIN_SRC mips
  loop:
	  lw          $4, 0($5)
	  sll         $7, $4, 1           #multiplication par 2
	  sw          $7, 0($5)    
	  lw          $14, 4($5)
	  sll         $17, $14, 1
	  sw          $17, 4($5)    

	  addiu       $5, $5, 4
	  bne         $9, $5, loop 
	  or          $0, $0, $0
#+END_SRC

Dans notre cas, on a seulement 2 instructions de gestion de boucle (on ne compte pas le NOP qui sera supprimé dans la suite) pour 6 et non plus 3 instructions de corps.

Dans cette version non encore optimisée, on a en revanche 3 cycles de gel et 1 instruction NOP.

De la même manière qu'avant, on peut supprimer les cycles et le NOP en réordonnançant. Sauf que cette fois, on a plus d'ordonnancement possibles, puisque le corps de la boucle est plus grand.

On doit prendre bien garde à respecter les dépendances :

#+BEGIN_SRC mips
  loop:
	  lw          $4, 0($5)
	  lw          $14, 4($5)
	  sll         $7, $4, 1           #multiplication par 2
	  sll         $17, $14, 1
	  addiu       $5, $5, 8
	  sw          $7, -8($5)    
	  bne         $9, $5, loop 
	  sw          $17, -4($5)  
#+END_SRC

On a bien 0 cycles de gel, et 0 instructions NOP.

On a 6 instructions de corps et 2 instructions de gestion de boucle : meilleur rapport.

Toutes ces techniques font bien mal à la tête ! On doit se donner un certain nombre d'outils théoriques qui permettront de formaliser correctement les possibilités d'optimisation, de manière à en évaluer leurs bénéfices et leur possibilité. (si on fait ça correctement, on pourra implémenter ces formalismes dans le compilateur, pour qu'il fasse les optimisations à notre place)

*** Optimisation et flot de contrôle

Si on prend un exemple un peu plus complexe :

#+BEGIN_SRC mips
  loop:
	  lw          $8, 0($5)
	  bgez        $8, endif
	  nop
	  sub         $9, $0, $8
	  sw          $9, 0($5)
  endif:
	  addiu       $5, $5, 4
	  bne         $7, $5, loop
	  nop
#+END_SRC

Dans notre cas, on a un branchement dans la boucle à l'instruction bgez (cas fréquent, traduit un simple if).

Quels problèmes pour l'ordonnancement et pour le déroulage de boucle ?

Il faut un des outils théoriques qui permettent de systématiser les réponses à cette question.

#+BEGIN_DEFINITION
Un *bloc de base* est une séquence d'instruction comportant un seul point d'entrée (la première instruction) et un seul point de sortie (la dernière), et qui suit la règle suivante :
Si la première instruction est exécutée, toutes les instructions du bloc de base le seront.
#+END_DEFINITION

#+BEGIN_THEOREM
On détermine les blocs de base d'un code de la façon suivante :

On détermine des en-têtes :

- La première instruction d'une fonction ou d'un bout de code
- L'instruction qui suit le dernier delayed slot après un saut
- L'instruction cible d'un saut

Une fois les en-tête placés, on peut définir les blocs de base de la manière suivante :
Du début à la fin du code, un bloc de base court d'une en-tête à la suivante exclue.
#+END_THEOREM

#+BEGIN_DEFINITION
Un graphe de contrôle de flot (CFG ou Control Flow Graph, dans la langue de 2Pac) est un graphe qui dessine les liens entre les blocs de base d'un programme. L'ensemble des liens constituent le contrôle de flot.

On dit qu'il y a un *arc* de BB1 vers BB2 deux blocs de base sssi :
- Soit il y a un saut de BB1 vers BB2
- Soit BB2 suit BB1 dans l'ordre du programme sans que BB1 ne se termine par un saut inconditionnel
#+END_DEFINITION

Si on reprend notre exemple :

#+BEGIN_SRC mips
					  #BB1
  loop:   
	  lw          $8, 0($5)
	  bgez        $8, endif
	  nop
					  #BB2
	  sub         $9, $0, $8
	  sw          $9, 0($5)
					  #BB3
  endif:  
	  addiu       $5, $5, 4
	  bne         $7, $5, loop
	  nop
#+END_SRC

L'ordonnancement doit prendre bien garde au flot de contrôle : on ne peut pas forcément bouger une instruction d'un bloc de base à un autre.

En revanche :

#+BEGIN_THEOREM
Si on a un seul bloc de base dans un code considéré, l'ordonnancement peut librement bouger n'importe laquelle instruction dans le bloc, pourvu que :
- Le code résultant respecte bien les dépendances
- Le code résultant soit bien toujours constitué d'un seul bloc de base
#+END_THEOREM

Dans le cas où on a plusieurs blocs de base, on doit faire plus attention aux arcs.

| BB1 | >-1 |
|-----+-----|
| V   |     |
|-----+-----|
| BB2 |     |
|-----+-----|
| V   |     |
|-----+-----|
| BB3 | <-1 |

Dans notre exemple :

- BB1 et BB3 sont toujours exécutés : on peut bouger des instructions de BB1 vers BB3 et vice versa, pourvu qu'on respecte les dépendances.
- BB2 n'est pas forcément exécuté : on ne peut pas mettre une instruction de BB1 dans BB2, ni remonter une instruction de BB3 de BB2
- On peut bouger des instructions dans BB2 (suit de la définition du bloc de base), et on peut *éventuellement* descendre des instructions de BB2 vers BB3, si on ne change pas la sémantique d'une itération.

Et comment fait-on quand on veut dérouler la boucle ?

#+BEGIN_SRC mips
					  #BB1
  loop:   
	  lw          $8, 0($5)
	  bgez        $8, endif
	  nop
					  #BB2
	  sub         $9, $0, $8
	  sw          $9, 0($5)
					  #BB1'
	  lw          $18, 4($5)
	  bgez        $18, endif
	  nop
					  #BB2'
	  sub         $19, $0, $8
	  sw          $9, 4($5)
					  #BB3
  endif:  
	  addiu       $5, $5, 4
	  bne         $7, $5, loop
	  nop
#+END_SRC

On a bien réécrit les blocs de contrôle.

On redessine le graphe :

| BB1  | >-1,2    |
|------+----------|
| V    |          |
|------+----------|
| BB2  |          |
|------+----------|
| V    |          |
|------+----------|
| BB1' | <-2, >-3 |
|------+----------|
| V    |          |
|------+----------|
| BB2' |          |
|------+----------|
| V    |          |
|------+----------|
| BB3  | <-1,3    |

On ne sépare pas un branchement interne de ses blocs successeurs.

On doit aussi faire attention à une dernière chose :

- Les instructions du corps de la boucle doivent être indépendantes les unes des autres, maintenant que leur indépendance n'est plus garantie par la boucle elle-même.
- On doit faire particulièrement aux modifications en avance.

En fait, le déroulage de boucle généralise la notion de "pipeline" : on traite des instructions de corps boucle à la pipeline : le traitement de la prochaine instruction de corps de boucle n'attend pas la prochaine itération de la boucle pour s'exécuter.


* Cours 5 : 17/10/2019

*** Rappels des épisodes précédents

Quelques rappels avant de voir la prochaine grande innovation des processeurs.

#+BEGIN_DEFINITION
La performance en temps d'exécution est donnée par la fréquence divisée par le CPI.
#+END_DEFINITION

#+BEGIN_THEOREM
On peut caractériser la dépendance de données de la manière suivante :

Si le cycle DEC d'un pipeline est situé avant l'écriture dans le banc de registre des registres qu'il doit lire, alors on a potentiellement un problème de dépendances de données.

Au contraire, si le cycle DEC est après cette écriture, on sait qu'on n'aura pas de problème de dépendances.

Si on a un pipeline à N étages, alors on peut avoir des problèmes de dépendances de l'instruction i à l'instruction i + n - 2 (on part du principe que DEC est le deuxième étage du pipeline). En particulier, dans notre cas d'un pipeline à 5 étages, on pourra avoir des problèmes de dépendances de données de i à i+3 au maximum.
#+END_THEOREM

#+BEGIN_THEOREM
Un cycle de gel augmente le nombre de cycles nécessaires à l'exécution d'un ensemble d'instructions de 1.
#+END_THEOREM

On a fondamentalement deux manières d'augmenter la performance, c'est d'augmenter la fréquence ou de diminuer le CPI.

L'idée du pipeline suit de la volonté d'augmenter la fréquence des processeurs. Les adaptations qu'on a vu en terme de bypass sont là pour préserver un CPI proche de 1.

On peut augmenter le fréquence, donc la performance en multipliant les étages de pipeline.

C'est l'invention du superpipeline.

*** Superpipeline

Mais comme on l'a vu avec le résultat plus haut, plus le pipeline est profond, plus on augmente le problème de la dépendance des données (celui-ci augmente linéairement en N le nombre d'étages du pipeline).

Augmenter le nombre de bypass n'est pas une solution miracle, car les bypass supposent un multiplexeur pour l'étage qui a le bypass en entrée, ce qui va avoir tendance à augmenter la durée du cycle, donc baisser la fréquence.

Autre problème, on augmente la quantité de delayed slots (CPI/CPI utile augmente) : si on dit que le décodage de l'adresse suivante se fait à la fin de l'étage N, on a N-1 delayed slots. (Déjà moins grave : on peut quand même partir du principe que l'étage DEC restera quoi qu'il en soit très près du début).

Si on interdit à l'utilisateur, donc au compilateur, de produire des branchements (pas de if ni de while autorisé), alors on peut se permettre de créer des processeurs avec beaucoup beaucoup d'étages de pipeline, qui seront efficaces pour le traitement des données peu dépendantes les unes des autres (typiquement, le calcul matriciel).

Le supercalculateur CRAY-2, de la fin des années 80 (1985).

*** Les processeurs SuperScalaire

On voit que les manières d'augmenter la fréquence en augmentant le nombre d'étages de pipeline posent presque autant de problème qu'elles en résolvent, et que les problèmes posés par cette solution n'augmentent pas de manière linéaire : à titre d'exemple, si un compilateur est assez capable de trouver une instruction à caser dans le delayed slot unique (75% du temps), ça devient beaucoup plus difficile avec 2 delayed slots (5%) et encore davantage avec 3 (0%).

Comment régler le problème donc ? Si on veut pouvoir continuer à augmenter la performance, mais que l'augmentation artificielle de la fréquence par le pipeline n'est pas assez rentable, il faut diminuer le CPI en dessous de ce qui était jusque là sa limite théorique : 1.

Autrement dit, il faut exécuter plusieurs instructions par cycle : invention des processus superscalaires, qui ont commencé à dominer dans les années 90. Les processeurs utilisés dans les machines modernes sont des processus superscalaires un peu particuliers (on le verra plus tard).

**** Aparté : les pipeline parallèles

L'idée d'avoir plusieurs pipeline n'est pas spécifique aux processeurs superscalaires. On sait que certaines opérations sont plus longues que d'autres : certains opérateurs comme l'addition, le shift, la comparaison bit à bit ont un temps de calcul logarithmique en le nombre de bits sur lequel ils sont appliqués, et certains autres (comme la multiplication) ne le sont pas du tout.

La manière naïve de faire une multiplication de deux nombres écrits sur 32 bits chacun est la manière de multiplier qu'on a appris à l'école : on fait les multiplications chiffre par chiffre (ici, bit à bit), puis on additionne le résultat des multiplications. On a au maximum 31 additions.

Un certain nombre d'algorithmes plus intelligents arrivent à faire ces multiplications en 3 additions plutôt qu'en 31. La multiplication reste quand même trois fois plus longue qu'une opération faite par l'unité logique et arithmétique.

On peut donc caser la multiplication à la places des 3 derniers étages du pipeline, en parallèle de ceux-ci.

**** Retour aux processus superscalaires

De la même manière que pour traiter la multiplication, on pourrait parfaitement doubler le pipeline d'exécution (E, M et W) pour en faire deux pipelines indépendants.

***** Conséquence sur l'accès mémoire et introduction du tampon d'instruction

Le cycle D devra donc décoder deux instructions à la fois.

Le cycle I doit maintenant alimenter le cycle decode avec deux instructions

La mémoire doit donc être capable de donner non pas 1, mais 2 instructions à la fois.

On rappelle que les instructions doivent être alignées, autrement dit que leurs adresses en octets doivent être divisibles par la taille en octets du mot d'instruction mémoire.

Ce qui suppose plusieurs choses :
- La largeur du bus CPU-mémoire centrale doit être doublée.
- Si on ne tient pas à mettre deux accès mémoire indépendants (ce qui coûte très cher), alors on doit demander au processeur de fetch deux mots d'instruction contigus d'un coup (les deux instructions pourront passer par le bus, mais ce sera toujours un seul bus, avec un seul contrôleur mémoire). Or, les mots d'instruction doivent être alignés (Adresses en octets divisibles par la taille du mot mémoire). L'adresse des couples d'instruction qu'on va demander à chercher devra donc être divisible par le double de la taille du mot mémoire. Si on veut que cette condition soit toujours respectée, on doit forcément :
-- Chercher toujours 2 mots d'instruction d'un coup
-- Ne jamais aller rechercher un mot d'instruction qu'on a déjà cherché avant (parmi les N qu'on va chercher) (donc ne jamais discard un mot d'instruction, à moins d'être sûr de ne point avoir envie de l'exécuter)

#+BEGIN_THEOREM
Problème généralisable à N : si on met N pipelines, le N-uplet d'instructions qu'on va chercher doit être divisible par N fois la taille du mot mémoire.

Si on veut que cette condition soit toujours respectée, on doit :
- Chercher toujours N mots d'instructions d'un coup
- Ne jamais aller rechercher un mot d'instruction qu'on a déjà cherché avant (parmi les N qu'on va chercher) (donc ne jamais discard un mot d'instruction, à moins d'être dûr de n'avoir point envie de l'exécuter)
#+END_THEOREM

Or, si les deux instructions passées chacune dans un des deux pipeline parallèle, il est parfaitement possible de ne pas pouvoir en exécuter une des deux (problème de dépendance). Or, d'après ce qu'on vient d'établir, on ne peut pas juste discard une instruction simplement parce qu'on ne peut pas l'exécuter : il faut être sûr de ne pas avoir envie du tout de l'exécuter, en raison par exemple d'un branchement conditionné ou non. Si on ne peut pas discard, il faut garder cette instruction non exécutée qqpart.

Cet endroit, c'est le *tampon d'instructions* (instruction buffer).

En fait, quand le matériel de l'étage IFC cherche deux mots d'instruction dans la mémoire, il place des deux mots d'instruction dans le tampon d'instruction, tampon dans lequel le matériel de l'étage DEC vient puiser une ou deux instructions (selon s'il peut en exécuter une ou deux).

#+BEGIN_THEOREM
Si on admet que le tampon ne peut être rempli que deux instructions par deux, et qu'on ne peut prélever que une ou deux instruction à la fois, quelle taille minimale du tampon garantit que l'étage DEC a toujours au moins deux instructions à prendre ?
4.

Résultat généralisable à N. Si on admet que le tampon ne peut être rempli que N instructions par N, et qu'on ne peut prélever que N, N-1, ..., 1 instruction à la fois, quelle taille minimale du tampon garantit que l'étage DEC a au moins N instructions à prendre ?
2N.
#+END_THEOREM

C'est la raison pour laquelle le tampon d'instruction peut tenir 4 instructions dans un processeur superscalaire à deux pipelines.

***** Conséquence sur les bypass

Rappels sur les bypass existant :
- E fin vers E début
- M fin vers E début
- E fin vers D début
- M fin vers D début

Donc 4 bypass par opérande sur un pipeline simple.

Si on imagine un superscalaire à deux pipelines, on doit potentiellement avoir deux 4^2 pipelines :
- Les 4 bypass de pipeline 1 vers pipeline 1
- Les 4 bypass de pipeline 1 vers pipeline 2
- Les 4 bypass de pipeline 2 vers pipeline 2
- Les 4 bypass de pipeline 2 vers pipeline 1

#+BEGIN_THEOREM
Si on admet le schéma de bypass classique du MIPS, alors le nombre de bypass nécessaire par opérande est donné par 4^N, N étant le nombre de pipelines du processeur superscalaire.
#+END_THEOREM

Dans le cas d'un processus superscalaire à deux pipelines, on a donc 16 bypass par opérande, donc 32 en tout.

**** Exemple

Prenons un exemple au tableau, reproduit ici :

#+BEGIN_SRC mips
  for:
	  lw          $6,0($4)
	  sll         $6,$6,1
	  sw          $6,0($4)
	  addiu       $4,$4,4

	  bne         $4,$9,for
	  nop
#+END_SRC

Faisons passer cette suite d'instructions dans un superscalaire à 2 pipelines, en faisant apparaître le contenu du tampon d'instructions.

| IB@endI  | Inst  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |   |   |
|----------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|          | lw    |   |   | E | M | W |   |   |   |   |    |    |   |   |
| lw,sll   |       | I | D |   |   |   |   |   |   |   |    |    |   |   |
|          | nop*  |   |   | E | M | W |   |   |   |   |    |    |   |   |
|----------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|          | sll   |   |   |   | 0 | E | M | W |   |   |    |    |   |   |
| sll,sw   |       |   | I | D |   |   |   |   |   |   |    |    |   |   |
| addiu    | nop*  |   |   |   | 0 | E | M | W |   |   |    |    |   |   |
|----------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|          | sw    |   |   |   |   |   | E | M | W |   |    |    |   |   |
| sw,addiu |       |   |   | I | 0 | D |   |   |   |   |    |    |   |   |
| bne,nop  | addiu |   |   |   |   |   | E | M | W |   |    |    |   |   |
|----------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|          | bne   |   |   |   |   |   |   |   | E | M | W  |    |   |   |
| bne,nop  |       |   |   |   | 0 | I | 0 | D |   |   |    |    |   |   |
| x,y      | nop   |   |   |   |   |   |   |   | E | M | W  |    |   |   |
|----------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|          | nop*  |   |   |   |   |   |   |   |   | E | M  | W  |   |   |
| x,y      |       |   |   |   |   |   |   | I | D |   |    |    |   |   |
| x1,y1    | nop*  |   |   |   |   |   |   |   |   | E | M  | W  |   |   |
|----------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|          |       |   |   |   |   |   |   |   |   |   |    |    |   |   |
| lw,sll   |       |   |   |   |   |   |   |   | I |   |    |    |   |   |
|          |       |   |   |   |   |   |   |   |   |   |    |    |   |   |


Le calcul du CPI et du CPI utile :

#+BEGIN_THEOREM
La bonne manière de calculer le nombre de cycles, donnée par Pirouz, est la suivante :

On note le numéro du cycle pendant lequel on fait rentrer la prochaine instruction effective qui n'est pas dans le segment étudié dans le pipeline. On note le numéro du cycle pendant lequel on fait rentrer la première instruction de notre segment. On fait la différence entre le premier et le second.

Cette méthode s'applique aussi bien aux itérations de boucle qu'aux suites séquentielles d'instructions : elle est en fait un peu plus robuste que la méthode de Karine, qui consistait à compter à partir de la sortie de la première instruction du segment jusqu'à la sortie de la dernière.

On peut aussi, dans le cas d'une boucle, compter le nombre de cycles en comptant la période à laquelle le tampon se vide à nouveau (si on part du principe que le tampon d'instruction est vide au début de notre exécution).
#+END_THEOREM

Dans notre exemple, la première instruction effective qui ne fait pas partie de notre segment étudié (une itération particulière de la boucle) rentre dans le pipeline au cycle 8. La première instruction de notre segment rentre dans le pipeline au cycle 1. On a donc bien 7 cycles.

On compte 6 instructions (attention, les nop* ne sont pas des instructions, mais des opérations, et ne comptent donc pas dans ce total), dont 5 utiles. Le CPI est donc de 7/6 et le CPI utile de 7/5. Ce n'est pas beaucoup mieux qu'un processeur pipeliné classique : ce processeur est particulièrement sensible aux branchements, comme on va le voir.

**** Opération nop*, flush du tampon d'instruction et efficacité

#+BEGIN_DEFINITION
On nomme *opération* l'instruction réduite à ses trois derniers étages. A titre d'exemple, on a défini nop*, qui est l'équivalent de l'instruction nop, qu'on met dans un des deux pipeline parallèles (de fait, dans le dernier des deux).
#+END_DEFINITION

Les opérations à mettre dans les différents pipelines ne sont en fait connues qu'à la fin de l'étage decode. Le matériel de l'étage decode regarde les deux premières instructions du tampon d'instruction, décide s'il peut mettre les deux dans le pipeline. S'il ne peut pas, il ne met que la première, et il met une opération nop* dans le deuxième pipeline parallèle.

Dans le cas spécifique d'un branchement, les 4 instructions du tampon d'instructions ne sont potentiellement plus les bonnes (décidé à la fin de l'étage decode de l'instruction de branchement). Il faut flush le tampon d'instructions. Or on sait que le tampon d'instructions doit être flush quand celui-ci est déjà plein de 4 instructions qu'on ne veut pas exécuter. C'est donc à l'instruction d'après seulement que des instructions à exécuter rentrent effectivement dans le tampon d'instruction.

On a donc, après un branchement réussi (si le branchement échoue, les instructions séquentielles sont valables), toujours une instruction inutile.

#+BEGIN_DEFINITION
Dans le cas considéré, on a une contrainte supplémentaire : une instruction ne peut pas, à la faveur du blocage d'une précédente, la doubler dans le pipeline. C'est ce qu'on appelle *in order execution*. C'est cette contrainte qui a été levée dans les processeurs modernes.
#+END_DEFINITION

#+BEGIN_THEOREM
Si on a N le nombre de pipelines dans notre processeur superscalaire, on peut exécuter n instructions (n > N) en au mieux (n/N) + 1 cycles.

Au pire, l'exécution prend autant de temps que dans un processus pipeliné classique (on ne peut passer d'instruction que dans le premier des N pipelines parallèles).
#+END_THEOREM

Ce type de processeur étant particulièrement sensible aux branchements (flush du tampon d'instruction obligatoire en cas de branchement réussi), on a intérêt à dérouler les boucles.

Réessayons avec deux itérations d'un coup.

**** Exemple, suite

#+BEGIN_SRC mips
  for:
	  lw          $8,0($4)
	  lw          $10,4($4)
	  addiu       $4,$4,8
	  sll         $8,$8,1
	  sll         $10,$10,1
	  sw          $8,-8($4)
	  bne         $4,$5,for
	  sw          $10,-4($4)
#+END_SRC

Cet exemple est déjà optimisé : on s'est permis de réordonnancer :

| IB@endI | Inst  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |   |   |
|---------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|         | lw    |   |   | E | M | W |   |   |   |   |    |    |   |   |
| lw,lw   |       | I | D |   |   |   |   |   |   |   |    |    |   |   |
|         | lw    |   |   | E | M | W |   |   |   |   |    |    |   |   |
|---------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|         | addiu |   |   |   | E | M | W |   |   |   |    |    |   |   |
| addiu   |       |   | I | D |   |   |   |   |   |   |    |    |   |   |
| sll     | sll   |   |   |   | 0 | E | M | W |   |   |    |    |   |   |
|---------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|         | sll   |   |   |   |   | E | M | W |   |   |    |    |   |   |
| sll     |       |   |   | I | D |   |   |   |   |   |    |    |   |   |
| sw      | nop*  |   |   |   |   | E | M | W |   |   |    |    |   |   |
|---------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|         | sw    |   |   |   |   |   | E | M | W |   |    |    |   |   |
| sw,bne  |       |   |   |   | I | D |   |   |   |   |    |    |   |   |
| sw      | bne   |   |   |   |   |   | E | M | W |   |    |    |   |   |
|---------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|         | sw    |   |   |   |   |   |   | E | M | W |    |    |   |   |
| sw      |       |   |   |   |   | I | D |   |   |   |    |    |   |   |
| x,y     | nop*  |   |   |   |   |   |   | E | M | W |    |    |   |   |
|---------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|         | nop*  |   |   |   |   |   |   |   | E | M | W  |    |   |   |
|         |       |   |   |   |   |   | I | D |   |   |    |    |   |   |
|         | nop*  |   |   |   |   |   |   |   | E | M | W  |    |   |   |
|---------+-------+---+---+---+---+---+---+---+---+---+----+----+---+---|
|         |       |   |   |   |   |   |   |   |   |   |    |    |   |   |
| lw,lw   |       |   |   |   |   |   |   | I |   |   |    |    |   |   |
|         |       |   |   |   |   |   |   |   |   |   |    |    |   |   |

Calcul du CPI et du CPI utile :

Nombre de cycles pour le segment concerné :
La première instruction effective après notre segment rentre dans le pipeline au cycle 7
La première instruction effective de notre segment rentre dans le pipeline au cycle 1

Notre segment prend donc 6 cycles pour s'exécuter.
On a 8 instructions, dont 8 instructions utiles, le CPI égale donc le CPI utile égale 0.75

[Vérifions quand même cette histoire de comptage des nop*]

On voit que le déroulage de boucle, parce qu'il permet de rendre le coût fixe d'une instruction perdue après les branchements réussis petit devant le coût total de la boucle déroulée, permet de bien exploiter les possibilités du superscalaire.

D'un autre côté, ce déroulage est nécessaire : avec la possibilité de faire passer deux instructions d'un coup dans le pipeline, les instructions se "rapprochent" les unes des autres, rendant les problèmes de dépendance plus aigus.

Un autre problème survient avec l'étage M : autant dans l'étage I, vu qu'on savait qu'on allait chercher N mots d'instructions contigus d'un coup, on pouvait se contenter de simplement élargir la nappe de fils qui devaient aller chercher les instructions, sans avoir besoin d'augmenter le nombre de contrôleurs mémoire.

Mais en ce qui concerne les lectures et les écritures dans la mémoire faites à l'étage M, on n'a aucune garantie que 2 écritures simultanées se fassent dans des octets/demi-mots/mots contigus (ni même que les écritures soient de la même taille, à dire vrai). On a donc deux solutions :
- Interdire deux écritures/lectures mémoire simultanées, ce qui implique soit d'arranger les instructions d'accès mémoire de manière à ce qu'elles ne soient pas dans le même N-uplet de mots d'instructions (de plus en plus complexe avec N augmentant), soit d'introduire des cycles de gel (jusqu'à N-1 dans le pire des cas), ce qui diminue grandement l'intérêt du superscalaire.
- Ajouter N contrôleurs à la mémoire de données, ce qui coûte extrêmement cher.

#+BEGIN_THEOREM
Quand on met un cycle de gel dans un des N pipeline séparés (en E, M ou W), on perd 1/N cycle (au lieu de 1 dans un pipeline classique).
#+END_THEOREM


* Cours 6 : 24/10/2019

Suite du premier cours de Karine : Rappels de déroulage de boucle, et introduction au pipeline logiciel.

** Rappels de déroulage de boucle

Prenons une boucle simple :

#+BEGIN_SRC mips
  loop:
	  lw          $4,0($5)
	  sll         $7,$4,1
	  sw          $7,0($5)

	  addiu       $5,$5,4
	  bne         $5,$9,loop
	  nop
#+END_SRC

Cette boucle lit 4 octets de mémoire dans un de ses registres, multiplie la quantité lue par deux, et réécrit le quantité modifiée à la même adresse.

Il incrémente ensuite l'adresse de 4 octets, vérifie qu'on n'est pas past-the-end dans la boucle, et reprend son exécution au début de celle-ci.

Dans cette boucle, le premier bloc de trois instructions constitue véritablement le corps de la boucle. Appelons-les i1, i2 et i3.

Les dépendances dans le corps de la boucle sont les suivantes :
- RAW de i2 vers i1, pour $4
- RAW de i3 vers i2, pour $7

Si on considère la prochaine itération de la boucle, on a aussi :
- WAR de i1(i+1) vers i2(i), pour $4
- WAR de i2(i+1) vers i3(i), pour $7

Ce sont les seules quatre dépendances qui concernent le corps de la boucle. Quand on arrange les instructions entre elles sur plusieurs itérations, on doit seulement prendre garde à garder les ordres suivants si on veut garder la sémantique en général :
- i1(i) -> i2(i)
- i2(i) -> i3(i)
- i2(i) -> i1(i+1)
- i3(i) -> i2(i+1)

Le déroulage de boucle suppose le renommage de certains registres. Si on y pense bien, les dépendances WAR peuvent assez facilement être supprimées en se contentant d'écrire dans un autre registre que celui dans lequel on lisait avant. C'est le principe du *déroulage de boucle* qu'on a vu il y a deux semaines.

** Introduction du pipeline logiciel

*** Sur notre exemple

Sans renommer de registres, et donc en respectant les dépendances WAR (les deux dernières de notre liste), on peut quand même dérouler la boucle.

Sur 5 itérations, (i = 0; i < 5; ++i), ça donne :

i1(0)
i2(0)
i1(1)

i3(0)
i2(1)
i1(2)

i3(1)
i2(2)
i1(3)

i3(2)
i2(3)
i1(4)

i3(3)
i2(4)
i1(4)

Pour les trois blocs non-extrêmes, on s'aperçoit qu'on a le même motif :

i3(i-2)
i2(i-1)
i1(i)

Puisque le motif est constant, on peut simplement reformer une nouvelle boucle !

#+BEGIN_DEFINITION
On vient d'exhiber le principe du pipeline logiciel : former une boucle avec des instructions appartenant à des itérations différentes.

L'idée rejoint celle du principe général du pipeline : on n'a pas besoin d'attendre la fin d'une itération avant de commencer les suivantes.
#+END_DEFINITION

Cette idée suppose en revanche une hypothèse très forte : les itérations doivent être indépendantes. L'opération d'intérêt (ici, la multiplication par une constante) ne doit pas dépendre des valeurs des itérations précédentes et suivantes. Par exemple, une moyenne mobile du style a[i] = 0.25a[i-1] + 0.5a[i] + 0.25a[i+1] ne fonctionnera pas bien.

*** Généralisation

Admettons qu'on puisse découper l'exécution en N étapes. On peut commencer l'étape M de l'itération i quand l'itération i-1 l'a terminée. Mais il faut aussi que les ressources nécessaires à l'étape M (les registres utilisés) aient été libérés : sinon, on écrase des valeurs pour l'itération i-1. En fait, il faut que les valeurs que l'itération i-1 a mis dans ces registres de fin d'étape M aient été traitées ou copiées ailleurs. Autrement dit, il faut que l'itération i-1 ait fini l'étape M+1.

On peut donc lancer l'étape M de l'itération i juste après l'étape M+1 de l'itération i-1 :

[[./CM6/pipeline.jpg][Enchaînement du pipeline]]

On voit qu'on a un motif qui se répète, comme dans notre exemple plus haut.

Le motif est le suivant :

E6(i-5)
E5(i-4)
E4(i-3)
E3(i-2)
E2(i-1)
E1(i)

[[./CM6/motif.jpg][Motif de la nouvelle boucle]]

La boucle ordonnance une étape de N itérations successives (N étant le nombre d'étapes du pipeline logiciel).

Pour le début et la fin, on doit quand même remplir et vider le pipeline.

*** Exemple complet

Prenons une boucle.

On a l'adresse mémoire a + i dans $6, et l'adresse mémoire a + N (past the end) en $10.

#+BEGIN_SRC mips
  loop:
	  lw          $4,0($6)
	  sll         $5,$4,1
	  sw          $5,0($6)
	  addiu       $6,$6,4
	  bne         $10;$6,loop
#+END_SRC

**** Première étape

Première étape : Déterminons dans la boucle ce qui fait partie du corps, et ce qui fait partie du contrôle de boucle en lui-même.

Le corps de la boucle consiste en les trois premières instructions, le contrôle en les deux dernières.

Analysons ensuite les dépendances (dépendantes de l'architecture) dans le corps de la boucle, et par rapport à la prochaine itération. (on suppose ici un MIPS à 5 étages)

- RAW de i2 vers i1 pour $4
- RAW de i3 vers i2 pour $5
- WAR de i1(i+1) vers i2(i) pour $4
- WAR de i2(i+1) vers i3(i) pour $5

Cycles perdus (dépend aussi de l'architecture, on continue à supposer un MIPS à 5 étages) :

- 1 cycle de gel entre i1 et i2 (on doit attendre la fin de M de i1 pour commencer E de i2)

C'est tout !

On coupe ensuite où il y a des cycles de gel, des dépendances, des limitations de performance. Dans notre cas, on va découper partout dans le corps de la boucle.

E1 : lw $4,0($6)
E2 : sll $5,$4,1
E3 : sw $5,0($6)

**** Deuxième étape

Le corps de la nouvelle boucle sera constitué de :

E3(i-2) : sw $5,?($6)
E2(i-1) : sll $5,$4,1
E1(i) : lw $4, ?($6)

On a besoin de modifier les adresses de lecture et d'écriture pour tenir compte du fait qu'on manipule plusieurs itérations de la boucle originale d'un coup.

#+BEGIN_THEOREM
Généralisable à N :

Soient E1, ..., En les étapes de découpage du pipeline logiciel.

On considère les itérations de i-n+1 à i, i la plus ancienne.

La nouvelle boucle sera formée des opérations :
- E_{n}(i-n+1)
- E_{n-1}(i-n+2)
...
- E_{2}(i-1)
- E_{1}(i)
#+END_THEOREM

Les instructions de contrôle de la boucle sont quand même exécutées à chaque itération, en particulier l'incrémentation de 4 de l'adresse.

Le corps de la nouvelle boucle, avec les adresses corrigées, sera donc :

E3(i-2) : sw $5,-8($6)
E2(i-1) : sll $5,$4,1
E1(i) : lw $4,0($6)

**** Troisième étape

On met la gestion de boucle, autant entre les itérations, qu'au début et à la fin de celles-ci.

On doit s'arrêter au bon moment : on ne doit pas faire de lecture d'adresses past the end.

La nouvelle boucle se fait sur la range suivante : (i = 2; i < N; ++i)
(ou alors (i = 0 ; i < N-2; ++i), comme vous voulez, la variable d'itération est muette)

On doit donc ajouter au début :
E1(0), E2(0), E1(1)

Et à la fin :
E3(N-2), E2(N-1), E3(N-1)

Ce qui donne, en brut non optimisé :

#+BEGIN_SRC mips
  remplissage:
	  lw          $4,0($6)            # Le remplissage du pipeline logiciel
	  sll         $5,$4,1
	  lw          $4,4($6)

	  addiu       $6,$6,8             # On commence la boucle à i = 2
  loop:
	  sw          $5,-8($6)
	  sll         $5,$4,1
	  lw          $4,0($6)

	  addiu       $6,$6,4
	  bne         $6,$10,loop
	  nop

  vidage:
	  sw          $5,-8($6)           # Le vidage du pipeline logiciel
	  sll         $5,$4,1
	  sw          $5,-4($6)
#+END_SRC

On a :
- 1 cycle de gel entre lw et sll, lignes 2 et 3
- 1 cycle de gel entre addiu et bne, lignes 12 et 13
- Un nop après bne, ligne 14

Optimisation :

- On peut faire remonter le addiu de la ligne 6, qui fait l'initialisation de la bonne adresse avant le début de la boucle du pipeline logiciel, entre le lw et le sll des lignes 2 à 3. Attention, cette opération requiert de changer l'immédiat de l'opération lw de la ligne de 4, de 4 à -4.
On se débarasse d'un cycle de gel.
- On peut faire passer le lw de la ligne 9 à la place du nop de la ligne 13. Attention, cette opération requiert de changer l'immédiat de l'opération lw de 0 à -4.
On se débarasse ainsi de l'opération nop
- On peut remonter l'instruction addiu ou bien entre sw et sll, ou bien même avant sw. Si on choisit cette deuxième manière, on doit changer l'immédiat de la l'opération sw, qui doit passer de -8 à -12.
De cette manière, on supprime le cycle de gel entre addiu et bne.

Ce qui donne :

#+BEGIN_SRC mips
  remplissage:
	  lw          $4,0($6)            # Le remplissage du pipeline logiciel
	  addiu       $6,$6,8             # On commence la boucle à i = 2
	  sll         $5,$4,1
	  lw          $4,-4($6)

  loop:
	  sw          $5,-8($6)
	  addiu       $6,$6,4
	  sll         $5,$4,1

	  bne         $6,$10,loop
	  lw          $4,-4($6)

  vidage:
	  sw          $5,-8($6)           # Le vidage du pipeline logiciel
	  sll         $5,$4,1
	  sw          $5,-4($6)
#+END_SRC

De cette manière, sur le corps de la boucle (label "loop"), on a un CPI égale au CPI utile, égale à 1.

*** Deuxième exemple complet

#+BEGIN_SRC mips
  loop:
	  lw          $4,0($6)
	  sll         $5,$4,1             # val * 2
	  add         $5,$5,$4            # val = val * 2 + val, soit val * 3
	  sw          $5,0($6)

	  addiu       $6,$6,4             # Contrôle de boucle
	  bne         $10,$6,loop
	  nop
#+END_SRC

Analyse du corps :

- RAW de sll à lw, pour $4 (1 cycle de gel)
- RAW de add à sll pour $5
- RAW de sw à add pour $5

Plus le cycle de gel entre addiu et bne, qui ne concerne pas le corps mais le contrôle.

On se propose de simplement découper le corps en 4 étapes.

La boucle pipelinée, avec son remplissage, et son vidage, mais brute :

#+BEGIN_SRC mips
  remplissage:
	  lw          $4,0($6)
	  sll         $4,$4,1
	  add         $5,$5,$4

	  lw          $4,4($6)
	  sll         $4,$4,1

	  lw          $4,8($6)

	  addiu       $6,$6,12            # On initialise i à 3

  loop:
	  sw          $5,-12($6)
	  add         $5,$5,$4            # Erreur ici : On est en train d'ajouter a[i-1] à 2a[i]
	  sll         $4,$4,1
	  lw          $4,0($6)

	  addiu       $6,$6,4
	  bne         $6,$10,loop
	  nop

  vidage:
	  sw          $5,-12($6)          # E4(N-3)

	  add         $5,$5,$4            # E3(N-2)
	  sw          $5,-8($6)           # E4(N-2)

	  sll         $4,$4,1             # E2(N-1)
	  add         $5,$5,$4            # E3(N-1)
	  sw          $5,-4($6)           # E4(N-1)
#+END_SRC

On voit tout de suite que la boucle est vérolée :

On ajoute a[i-1] à 2a[i] dans le registre $5, alors qu'on veut ajouter a[i] à 2a[i].

E1 produit $4 utilisé en E2 et E3
E2 produit $5 utilisé en E3
E3 utilise $5 et $4 et produit $5 pour E4

Le problème vient de ce qu'un étage produit des registres pour plusieurs autres étages à la fois, et donc de ce qu'un étage utilise un registre qui doit être utilisé par un autre. Certains registres sont écrasés avant d'être utilisés par tous les étages qui voulaient s'en servir.

#+BEGIN_THEOREM
Dans une boucle pipelinée, pour que la boucle soit correcte, on doit respecter ces deux conditions :
- Chaque étape doit produire une valeur dans un registre différent des autres (sinon, on doit renommer des registres)
- Chaque étape doit uniquement utiliser des valeurs de l'étape précédente, et produire pour l'étape suivante.
#+END_THEOREM

#+BEGIN_PROOF
La première condition est évidente.

L'évidence de la deuxième condition apparaît avec davantage de clarté et d'évidence quand on refait le parallèle avec le pipeline matériel : quand on voulait transmettre une valeur sur plusieurs étages, on était obligé d'utiliser un registre de pipeline différent à chaque étage. On ne peut pas copier un registre dans lui-même, on doit bien se rappeler le fait que l'instruction derrière nous veut écrire dans le même registre que nous. On ne peut pas non plus écrire dans le même registre dans lequel l'instruction devant nous veut écrire.

Ici, les registres intermédiaires de pipeline doivent être remplacés par des registres visibles du banc de registre, mais les mêmes règles doivent être respectées.
#+END_PROOF

On va donc rectifier notre boucle, en allant de E1 vers E4 (donc du bas vers le haut) :

#+BEGIN_SRC mips
  remplissage:
	  lw          $4,0($6)            # E1(0)

	  sll         $5,$4,1             # E2(0)
	  ori         $9,$4,0
	  lw          $4,4($6)            # E1(1)

	  add         $7,$5,$9            # E3(0)
	  sll         $5,$4,1             # E2(1)
	  ori         $9,$4,0
	  lw          $4,8($6)            # E1(2)

	  addiu       $6,$6,12            # On initialise i à 3

  loop:
	  sw          $7,-12($6)
	  add         $7,$5,$9
	  ori         $9,$4,0
	  sll         $5,$4,1
	  lw          $4,0($6)

	  addiu       $6,$6,4
	  bne         $6,$10,loop
	  nop

  vidage:
	  sw          $7,-12($6)          # E4(N-3)
	  add         $7,$5,$9            # E3(N-2)
	  ori         $9,$4,0             # E2(N-1)
	  sll         $5,$4,1

	  sw          $7,-8($6)           # E4(N-2)
	  add         $7,$5,$9            # E3(N-1)

	  sw          $7,-4($6)           # E4(N-1)
#+END_SRC

On a du rajouter une instruction de transmission pure. On renomme le remplissage et le vidage du pipeline exactement comme on a renommé le corps de la boucle pipelinée.

On pourrait s'amuser à optimiser cette série d'instructions, mais ce n'est pas l'objet ici.

*** Les dépendances de contrôle

Les choses se compliquent encore quand on rajoute des dépendances de contrôle dans la boucle à dérouler.

Deux exemples :

#+BEGIN_SRC mips
  loop:
	  lw          $3,0($4)
	  bgez        $3,suite
	  nop
	  sub         $3,$0,$3
	  sw          $3,0($4)
  suite:
	  addiu       $4,$4,4
	  bne         $4,$10,loop
	  nop
#+END_SRC

#+BEGIN_SRC mips
  loop:
	  lw          $3,0($4)
	  bgez        $3,else
	  nop
	  addi        $3,$3,-1
	  j           suite
  else:
	  addi        $3,$3,1
  suite:
	  sw          $3,0($4)
	  addiu       $4,$4,4
	  bne         $4,$10,loop
	  nop
#+END_SRC

On s'entraîne à définir les blocs de base des deux exemples précédents.

Exemple 1 :
- 1 en-tête avant lw (début)
- 1 en-tête après nop (post-branchement)
- 1 en-tête avant addiu (cible d'un branchement)

Exemple 2 :
- 1 en-tête avant lw (début)
- 1 en-tête après nop (post-branchement)
- 1 en-tête avant le deuxième addi (cible d'un branchement)
- 1 en-tête avant sw (cible d'un branchement)

A partir de ces blocs de base, on peut dessiner le diagramme de flot de contrôle :

[[./CM6/controle1.jpg][Diagramme de flot de contrôle 1]]

Où peut-on couper l'itération ?

#+BEGIN_THEOREM
Un branchement ne peut pas être séparé des instructions vers lesquelles il fait peut-être sauter.

Si une instruction est conditionnée à une autre, alors elle doit nécessairement être mise dans le même étage de pipeline logiciel que celle-là dont elle dépend.
#+END_THEOREM

Donc, on ne pourra couper que de cette manière-là (on ne pourra pas découper dans les carrés en pointillé, mais uniquement entre le contenu du carré d'une part, et qqch en dehors de celui-ci, d'autre part) :

[[./CM6/controle2.jpg][Diagramme de flot de contrôle découpé]]

*** Conclusion

Pour résumer, quand on veut optimiser une boucle, on peut appliquer le principe du pipeline logiciel, qui fonctionne selon la même idée que le pipeline matériel : on n'a pas besoin d'attendre qu'une instruction soit terminée pour lancer la suivante. Les mêmes contraintes cependant doivent être respectées.

Le principal intérêt est qu'on renomme moins de registres que le simple déroulage de boucle, qu'on se débarrasse des dépendances RAW, et donc des cycles de gel qui s'ensuivent, au prix minime de davantage de dépendances WAR, et aussi et surtout qu'on recomprime la taille du segment de texte du processus, ce qui est excellent pour la performance du cache.

Les règles à retenir :

On ne considère que le corps de la boucle, pas les instructions de contrôle de la boucle.

On ne sépare jamais les instructions dépendantes d'un branchement de ce branchement dont elles dépendent.

On coupe là où il y a des pertes de performance liées à des dépendances RAW et des cycles de gel.

On vérifie que, comme dans le pipeline matériel :
- Chaque étage produit dans un registre différent
- Chaque étage consomme exactement du registre produit par l'étage d'avant, et produit exactement dans un unique registre, consommé uniquement par l'étage d'après : si besoin, on rajoute des instructions de transfert dans un registre libre à l'intérieur de l'étape.

Pour que cette condition soit respectée, on peut aussi revoir le découpage : dans notre exemple plus haut, laisser :

#+BEGIN_SRC mips
	  sll         $5,$4,1
	  add         $5,$5,$4
#+END_SRC

ensemble dans un même étage permet de respecter automatiquement la condition. Cet étage est une boîte noire, qui ne lit qu'un seul registre, $4, et qui n'en produit qu'un seul, $5, le second étant le triple du premier.

En plus, dans ce cas, on n'avait même pas de cycle de gel qui justifiât un découpage.

Découper entre ces deux instructions nous a forcé à rajouter une instruction de copie de registre, ce qui est dommageable pour les performances. Si on ne coupait pas, on se trouvait avec une boucle de pipeline plus courte d'une instruction, sans davantage de cycles de gel ou de nop.

Le pipeline logiciel, contrairement au pipeline matériel, est *flexible sur le nombre d'étages en lequel il est découpé*. Autant on ne peut pas redécouper un pipeline MIPS à la volée quand ça nous arrange, selon les instructions du moment, autant on peut le faire avec un pipeline logiciel, pourvu que les conditions mentionnées plus haut soient respectées. Utilisons cette flexibilité à notre avantage, soyons économes sur les découpages là où nous pouvons nous le permettre.


* Cours 7 : 14/11/2019

Ce cours commence la partie sur la hiérarchie mémoire qui durera jusqu'à la fin du cours.

Les deux composants essentiels d'un ordinateur sont le processeur central et la mémoire centrale.

Au fur et à mesure qu'on rend les processeurs de plus en plus puissants avec les innovations qu'on a vu pendant la première partie les contraintes posées par le processeur sur la mémoire sont de plus en plus fortes.

Les contraintes posées par le processeur sur la mémoire :
- 1 ou 2 accès par cycle
- temps de cycle de la mémoire = temps de cycle du processeur
- On veut un espace d'adressage de 4Go * le nombre de processus dans la machine (mémoire large)
- Coût raisonnable

Il n'existe pas, à ce jour, de mémoire qui respecte ces quatre conditions à la fois. On peut en revanche concevoir plusieurs types de mémoire qui respectent chacun un sous-ensemble de ces contraintes.

** Les différents type de mémoire

#+BEGIN_DEFINITION
Mémoire statique vs mémoire dynamique

Mémoire statique : information est un bit, enregistré de manière statique. On a un circuit électronique qui maintient activement cette information. Un dispositif actif (deux fois deux transistors qui constituent deux inverseurs tête-bêche, garantissant que la donnée garde sa valeur malgré un bruit tolérable) maintient cette valeur. On utilise les mêmes portes logiques que le processeur, on a donc les mêmes performances. Chaque bit nécessite au moins 6 transistors (4 pour les deux inverseurs, qui sont une autre manière de nommer les portes logiques not, plus 2 pour relier les inverseurs aux lignes de données).

[[./CM7/sram.png][Mémoire statique]]

Mémoire dynamique : stocke aussi un bit (même s'il est possible de stocker plus, avec les technologies actuelles). La donnée est stockée de manière passive dans une capacité (ou un capaciteur, ou un condensateur) : une tension gardée correspond à une valeur de bit. Si on est capable de stocker de manière fiable plus que deux valeurs de tension, alors on peut stocker deux voire trois bits de donnée (ce qui coûte donc respectivement une précision de quatre et huit valeurs aisément distinguables). Déconnecter la capacité permet de figer les valeurs de tension connectées dans cette capacité. On a par contre un phénomène de fuite, ce qui fait que les données doivent être rafraîchies régulièrement (toutes les ms).

[[./CM7/dram.png][Mémoire dynamique]]
#+END_DEFINITION

*** La mémoire statique

La mémoire statique respecte la première et la deuxième condition : elle est d'accès très rapide.

Ce matériel est fabriqué en silicium. Il ressemble un peu aux registres du processeur, il est d'ailleurs fabriqué avec les mêmes technologies.

Par contre, coûte cher : 6 transistors par bit.

Si on essaie de faire une mémoire pas chère avec cette technologie, on a à peine quelque Ko à quelques Mo.

*** La mémoire dynamique

Un autre mémoire, c'est la mémoire dynamique :
3-10 cycles par accès.

Silicium aussi. On peut avoir quelques Mo à quelques Go à un prix raisonnable.

*** Le disque dur

Le disque dur : pour un prix raisonnable, de quelques Go à plusieurs To.

Pas du silicium, du matériel métallique magnétique.

Accès mécanique.
Quelques ms d'accès (10ms, soit 10 millions de cycles)

*** La bande magnétique

Pour un prix raisonnable, quelques To à plusieurs Po.
10 minutes d'accès : accès séquentiel, il faut mettre la bobine au bon endroit avant d'accéder aux données.
Encore moins cher : plastique

*** Retour à nos contraintes

On le voit immédiatement, aucun de ces quatre matériels ne respecte les quatre contraintes qu'on a énoncé plus haut. Par contre, chacune d'entre-elle en respecte un sous-ensemble.

La solution intuitive est de mélanger les différents types de mémoire. Le processeur est censé s'adresser successivement aux différentes mémoires, de la plus rapide à la moins rapide.

*** Exemple

On suppose une mémoire statique avec temps d'accès d'un cycle, de 4 Ko, une mémoire dynamique qui demande 10 cycles pour un accès, et propose 4 Mo, et un disque dur qui réclame 1 million de cycles par accès, et propose 4 Go.

Si on suppose que les processeur demande à accéder à une adresse choisie selon une loi uniforme sur [0, 4*10^9], alors l'espérance du nombre de cycles pour un accès mémoire sera en fait très proche de 10^6.

Le calcul exact est donné par E = 1 * (10^-6) + 10 * (10^-3 - 10^-6) + 10^6 * (1 - 10^-3).

Bien heureusement, le processeur ne demande pas d'accéder à des adresses choisies selon une loi uniforme sur la totalité de l'espace d'adressage donné par le disque dur.
On peut en fait supposer deux propriétés sympathiques, et ce de manière assez universelle en le type du programme :
- La localité spatiale : les adresses demandées dans un programme sont proches les unes des autres : les instructions sont placées de manière séquentielle dans des boucles ou des fonctions, les données sont placées sur une pile, ou avec un peu de chance en tous cas de manière contigüe.
- La localité temporelle : la probabilité d'accéder à une adresse proche de la dernière adresse accédée est plus élevée que la probabilité d'accéder à une adresse proche d'une adresse accédée il y a 10000 cycles.

Ces deux propriétés doivent être exploitées, si on veut pouvoir avoir besoin de moins d'un million de cycles en moyenne pour accéder à un octet : l'idée est de ramener plus que la donnée d'intérêt des mémoires lentes au mémoire rapides, de ramener un *bloc mémoire* autour de cette donnée. Ce bloc fait en général quelques dizaines d'octets (32, 64, 128).

On peut exploiter ça en ramenant plus que les données d'intérêt dans les mémoires plus rapides : on ramène un *bloc mémoire* (quelques dizaines d'octets : 32, 64, 128).

A partir de maintenant, la première mémoire statique sera appelée *cache*, la deuxième mémoire dynamique sera appelée *RAM*, ou *mémoire centrale*, ou *mémoire primaire*, le disque dur sera la *mémoire secondaire*. On oublie la mémoire magnétique pour le moment, elle n'est pas utilisée dans les ordinateurs habituels.

Le processeur est censé demander d'abord au cache si un octet ou un mot s'y trouve. Si l'octet ou le mot ne s'y trouve pas, le bloc entier est alors ramené de la mémoire centrale vers le cache.

Le temps que cela prend se calcule de la manière suivante :

On part du principe qu'on a des blocs de 32 octets, et que la largeur du bus est d'un mot mémoire.
Ramener les 8 mots mémoire (on est dans une architecture 32 bits) de la mémoire vers le cache prend donc 80 cycles.

Si on part du principe que le cache miss a une probabilité de 0.1, alors l'espérance du temps d'accès à l'octet ou au mot mémoire est donnée par :
E = 0.9 * 1 + 0.1 * 10 * 8 = 8.9

C'est assurément mieux que 1 million, mais c'est encore trop, d'autant plus qu'on a fait une hypothèse assez généreuse sur la proba du cache miss.





En élargissant la nappe de fils de la mémoire vers le cache, on réduit énormément le nombre de cycles. Coûte cher, mais très efficace.

Analysons plus précisément les 10 cycles nécessaire pour l'accès à la mémoire : en fait, une partie de ces 10 cycles est due :
- Au décodage de l'adresse
- À l'attente pour l'accès au bus système

#+BEGIN_DEFINITION
Le bus système

Le bus système est un système de communication entre les différents périphériques d'une machine, partagé entre plusieurs (plus de deux) périphériques. On distingue ce système de la liaison de point à point, à l'usage exclusif des deux matériels situé à chaque extrémité. On a potentiellement, à chaque moment, plusieurs éléments qui désirent accéder au bus pour écrire des octets à destination d'autres éléments. Comme le bus, qui est en fait un fil, ne peut transmettre qu'une information "à la fois", disons sur une unité atomique de temps et d'espace à déterminer, il ne peut en fait être utilisé que par un matériel à la fois. Cette contrainte suppose l'existence d'un *arbitre de bus*, ou d'un *contrôleur de bus*, une espèce d'ordonnanceur d'accès au bus.

La vitesse du bus système est un des goulets d'étranglement les plus importants de la puissance d'un ordinateur, c'est aussi une des grandeurs les plus coûteuses à faire passer à l'échelle.
#+END_DEFINITION

Donc, indépendemment de la quantité de données qu'on va chercher, on doit toujours payer un temps forfaitaire pour le décodage de l'adresse plus l'accès au bus.

Le temps d'accès total est donc donné par :

T = A + B * (Taille bloc en mots / Taille bus en mots)

Avec A qui désigne le forfait, et B le coût du transfert d'un mot mémoire.

En général, pour des raisons qu'on ne rappellera pas ici, A domine très largement B. C'est précisément le fait que le forfait est très cher devant le coût du transfert que les transferts en gros, et donc les tampons et les caches, sont beaucoup beaucoup plus efficace que les transferts au détail.

Si on suppose dans notre exemple que A = 9 et B = 1, et si on suppose toujours les mêmes probabilités de cache miss et de cache hit, et si on suppose encore que la taille du bus est de 4 octets, de même que la taille du bloc est de 32, alors le temps espéré d'accès à l'octet ou au mot d'intérêt est donné par :
E = 0.9 * 1 + 0.1 * (9 + 1 * 8) = 0.9 + 0.9 + 0.8 = 2.6

Ce qui est beaucoup mieux : 2.6 cycles en moyenne pour accéder à un mot, en supposant une proba de miss de 0.1, c'est tout-à-fait acceptable.

** Organisation des mémoires

Le cache et la mémoire sont organisés en blocs, pas en mots.

Comment sont organisées les données dans le cache et la mémoire centrale ?

On dit que la mémoire est divisée en blocs, et que le cache est divisé en blocs, ou plutôt en emplacements qui permettent d'accueillir des blocs. Bien entendu, les blocs de la mémoire centrale font la même taille que les blocs, ou emplacements de blocs, du cache.

Dans la suite, on nommera blocs des blocs de la mémoire centrale et emplacements les emplacements de blocs dans le cache. On a bien entendu bien plus de blocs que d'emplacements.

On distingue trois types de cache quand à la correspondance entre les blocs et les emplacements :
- Direct mapping (chaque numéro de bloc peut atterrir à un et un seul emplacement)
- Set associative (chaque numéro de bloc peut atterrir dans un ensemble d'emplacements)
- Full associative (chaque numéro de bloc peut atterrir où il veut)

On voit immédiatement que le dernier des trois types est le meilleur, il nous laisse la plus grande liberté dans les blocs qu'on pourra effectivement avoir dans le cache à un moment arbitraire. C'est aussi potentiellement le plus coûteux en matériel et en overhead.

*** Le direct mapping

Correspondance la plus simple.

Dans la mesure où on a bien plus de blocs que d'emplacements, on doit partitionner l'ensemble des blocs d'une certaine manière à pouvoir à chaque partition associer un emplacement dans lequel tous les blocs de la partition iront se loger.

La partition la plus intelligente, si on se rappelle les propriétés de localité spatiale et temporelle des programmes, c'est la partition telle que la distance minimale entre deux blocs devant se loger dans un même emplacement soit maximale égale à la taille du cache en nombre d'emplacements.

En effet : on voudrait que si un bloc n doit se loger dans un emplacement m, les blocs n+1 et n-1 puissent chacun se loger dans deux emplacements distincts deux à deux et différents de m. Sinon, il ne sera pas possible d'avoir à la fois les blocs n-1, n, n+1 dans le cache, ce qui est une source sûre de gâchis étant donné les propriétés de localité spatiale et temporelles des programmes. Ce raisonnement reste valable tant qu'il reste des emplacements dans le cache.

#+BEGIN_DEFINITION
Optimisation de la partition :

Le critère d'optimisation des partitions est donné par la plus grande série de blocs contigus de la mémoire centrale qu'une partition permet au cache de stocker à n'importe quel moment. Plus simplement dit, plus une partition permet à un cache de stocker au même instant une plus grande série de blocs contigus, meilleure elle est. De surcroît, la série de blocs contigus de taille maximale est donné par la taille du cache en nombre d'emplacements.
#+END_DEFINITION

#+BEGIN_THEOREM
Théorème :

Soit M la taille du cache en nombre d'emplacements, emplacements indexés de 0 à M-1, et N la taille de la mémoire centrale en blocs, blocs indexés de 0 à N-1. La partition qui associe le bloc i à l'emplacement j, avec i congruent à j modulo M, est optimale selon le critère donné plus haut.
#+END_THEOREM

#+BEGIN_PROOF
La démonstration est évidente, et suit de l'exemple pris plus haut. On pourra aussi se refaire le dessin des colonnes présenté par Pirouz.
#+END_PROOF

**** Implantation matérielle

A quoi ressemble le cache direct mapping du point de vue matériel ?

On part toujours du principe que les blocs font 32 octets, et que la taille du mot mémoire est de 32 bits.

Etant donné ce matériel, à quoi ressemble une adresse mémoire pour les différents composants. On sait déjà que l'adresse s'écrit sur 32 bits.

Pour la mémoire principale, les 5 bits de poids faible vont désigner le décalage en octets depuis le début du bloc (car les blocs font 32 octets). Les bits de poids fort donnent le numéro du bloc.
Si on admet une taille de bloc à 64 (respectivement 128 octets), on devra attribuer les 6 (respectivement 7) octets de poids faible au décalage depuis le début du bloc. Mais puisqu'il y aura 2 (respectivement 4) fois moins de blocs (on a toujours au maximum 2^32 adresses), on pourra quand même nommer toutes les octets.

Pour le cache, c'est un peu plus compliqué : les 5 bits de poids faible vont toujours désigner le décalage en octets par rapport au début du bloc, mais les bits de poids fort seront séparés en les bits qui serviront à donner le numéro de l'emplacement, et en les bits qui serviront à donner *l'étiquette* du bloc dans l'emplacement. Si on admet que le nombre d'emplacements dans un cache est une puissance de 2 (et ça doit être le cas, sinon c'est du gâchis de bits), alors le nombre total de bits utilisés pour écrire le numéro de l'emplacement est donné par log_2(nombre d'emplacements), qui est une quantité entière.

L'étiquette du bloc est donc un nombre écrit sur (taille du mot mémoire) - log_2(nombre d'emplacements du cache) - log_2(taille du bloc en octets) bits.

Le cache est séparé en deux parties, la partie *directory* qui stocke les étiquettes des blocs et la partie *data*, qui stocke les données des blocs. Chaque partie a donc le même nombre d'emplacements, le nombre d'emplacements du cache donné plus haut.

Dans la partie directory du cache, on a un bit de présence pour chaque emplacement, qui dit si l'emplacement contient ou non un bloc.

Quand le processeur central demande une adresse, celle-ci est formatée de la même manière que pour le cache : en effet, le processeur ne parle qu'à son cache pour le moment. Le fait que la notion d'étiquette ou d'emplacement, ou de bloc, ou de décalage par rapport au bloc n'aient pas de sens pour le processeur n'est pas un problème ici : il se trouve simplement que l'adresse demandée par le processeur à son cache se conforme au formatage donné plus haut pour le cache. Le contrôleur du cache compare les (taille du mot mémoire) - log_2(nombre d'emplacements du cache) - log_2(taille du bloc en octets) (soit ici 24) bits de poids fort de l'adresse envoyée par le processeur aux 24 bits stockés dans la partie directory de l'emplacement donné par les bits 24 à 26 (dans notre exemple à 8 emplacements) de l'adresse envoyée par le processeur. Si ces 24 bits-ci sont les mêmes que ces 24 bits-là, et que le bit de présence idoine est à 1, alors on sait que le contenu de l'emplacement est valable.

Dans ce cas, le cache envoie les données au processeur. Quand une au moins de ces conditions n'est pas respectée, soit le bit de présence idoine à 0, soit une non-correspondance entre les 24 bits de poids fort de l'adresse envoyée par le processeur, et les 24 bits stockés dans l'emplacement idoine de la partie directory du cache, que se passe-t-il ?

Les données sont quand même envoyée, *mais* par un autre fil est envoyé un bit de "miss" qui est mis a 1 quand une de ces conditions n'est pas respectée : ce bit de miss dit simplement au processeur de ne pas tenir compte des données envoyée.

Cette mise en place, qui semble contre-intuitive (pourquoi ne pas simplement conditionner l'envoi des données à leur validité ?), est en fait la moins coûteuse en matériel : on envoie les données sans se poser de question, et au prix seulement d'un comparateur et d'une porte logique NAND, on indique si les données sont valables. *Ce coût de la validation est constant en la quantité de données effectivement envoyées au processeur*, et promet donc de bien passer à l'échelle.

[[./CM7/cache.png][Illustration du cache]]

* Annexes

Support de cours :

[[./CM4/cours4.pdf][Cours Karine 1]]
[[./CM6/cours6.pdf][Cours Karine 2]]


